{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8108a0e2",
   "metadata": {},
   "source": [
    "# 1. Generation of Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db6a6254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lauraluckert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lauraluckert/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import wikipediaapi\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import requests\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412ad8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"~/Desktop/\"\n",
    "FILENAME = \"tmdb_5000_credits.csv\"\n",
    "\n",
    "full_path = os.path.expanduser(PATH)\n",
    "os.chdir(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888dc60",
   "metadata": {},
   "source": [
    "### 0. Example Structure for New Game Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d2c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_field = {\n",
    "    \"streets\": {\"1-3\": [],\"4-6\": [],\"7-9\":[] , \"10-12\": [], \\\n",
    "                \"13-15\": [], \"16-18\": [], \"expensive\": [], \"cheap\": []},\n",
    "    \"stations\": [],\n",
    "    \"prison\": [],\n",
    "    \"free_parking\": [],\n",
    "    \"special\": {\"1\": [], \"2\": []}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346385af",
   "metadata": {},
   "source": [
    "### 1. Clean and read-in Movie Data\n",
    "Data Source:\n",
    "https://www.kaggle.com/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a75918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_movie_dataset(movie_data):\n",
    "    \"\"\"\n",
    "    :movie_data: Pandas DataFrame holding movie titles, character cast\n",
    "    Preprocess the data, we only need the characters from the movie in a dict\n",
    "    \n",
    "    :returns: dictionary with key = movie title, value = \n",
    "    \"\"\"\n",
    "    \n",
    "    cast_rows = []\n",
    "\n",
    "    for malformed_string in movie_data.cast:\n",
    "        imd_string = list(malformed_string[1:(len(malformed_string)-1)].split(\"}\"))\n",
    "    \n",
    "        new_list = []\n",
    "\n",
    "        for item in imd_string:\n",
    "            try: \n",
    "                if item[0] != \"{\":\n",
    "                    item = item[2:(len(item))]\n",
    "                item += \"}\"\n",
    "                new_item =json.loads(item)\n",
    "                person = new_item[\"character\"]\n",
    "                #gender = new_item[\"gender\"]\n",
    "                new_list.append(person)\n",
    "            except IndexError:\n",
    "                break\n",
    "        cast_rows.append(new_list)\n",
    "\n",
    "    \n",
    "    cast_dict = {}\n",
    "    for movie, cast in zip(movie_data.title,cast_rows):\n",
    "        cast_dict[movie] = cast\n",
    "    \n",
    "    return cast_dict\n",
    "\n",
    "## preprocess dataset\n",
    "movie_characters = pd.read_csv(FILENAME, sep=\",\")\n",
    "cast_dict = clean_movie_dataset(movie_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915998c",
   "metadata": {},
   "source": [
    "### 3. Random Selection of Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2908946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Impact\n"
     ]
    }
   ],
   "source": [
    "## get random movie from our dictionary\n",
    "random_key = random.choice(list(cast_dict))\n",
    "print(random_key)\n",
    "## topic and cast is selected\n",
    "#topic = random_key\n",
    "#cast = cast_dict[topic]\n",
    "\n",
    "## for testing purposes, we set the topic manually\n",
    "#topic = \"Furious 7\"\n",
    "#cast = cast_dict[topic]\n",
    "#print(topic, cast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74155c76",
   "metadata": {},
   "source": [
    "### 4. Select Characters as New Locations\n",
    "#### 4.1 *To Do*:  Cleaning of character names (no brakets in names etc.)\n",
    "#### 4.2 *To Do*: Useful combination of streetnames with selected characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59aa6b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'streets': {'1-3': ['Mr. Nobody Street', 'Deckard Shaw Drive', 'Han Drive'], '4-6': ['Sean Boswell Lane', 'Elena Boulevard', 'Hector Road'], '7-9': ['Owen Shaw Drive', 'Safar Road', 'Jack Lane'], '10-12': ['Samantha Hobbs Alley', 'Letty Fan Park', 'Female Racer Park'], '13-15': ['Race Starter Lane', 'Hot Teacher Drive', 'Doctor Park'], '16-18': ['Merc Tech Road', 'Weapons Tech Lane'], 'expensive': ['Dominic Toretto Avenue', \"Brian O'Conner Avenue\"], 'cheap': ['Kiet Drive', 'Kara Drive']}, 'stations': ['Letty Station', 'Roman Station', \"Tej (as Chris 'Ludacris' Bridges) Station\", 'Mia Station'], 'prison': [], 'free_parking': [], 'special': {'1': [], '2': []}}\n"
     ]
    }
   ],
   "source": [
    "## possible street names for combination with characters\n",
    "street_names = [#'Avenue', \n",
    "                'Park', 'Street', 'Boulevard', 'Road', 'Main Street', 'Drive', 'Lane', 'Alley']\n",
    "\n",
    "## fill location entries with characters from film cast (cast_dict)\n",
    "## example default combination\n",
    "new_field[\"streets\"][\"expensive\"] = [x + \" Avenue\" for x in cast[0:2]]\n",
    "new_field[\"streets\"][\"cheap\"] = [x + \" Drive\" for x in cast[8:10]]\n",
    "new_field[\"streets\"][\"1-3\"] = [x + \" \" + random.choice(street_names) for x in cast[11:14]]\n",
    "new_field[\"streets\"][\"4-6\"] = [x + \" \" + random.choice(street_names) for x in cast[15:18]]\n",
    "new_field[\"streets\"][\"7-9\"] = [x + \" \" + random.choice(street_names) for x in cast[19:22]]\n",
    "new_field[\"streets\"][\"10-12\"] = [x + \" \" + random.choice(street_names) for x in cast[23:26]]\n",
    "new_field[\"streets\"][\"13-15\"] = [x + \" \" + random.choice(street_names) for x in cast[27:30]]\n",
    "new_field[\"streets\"][\"16-18\"] = [x + \" \" + random.choice(street_names) for x in cast[31:33]]\n",
    "new_field[\"stations\"] = [x + \" Station\" for x in cast[3:7]]\n",
    "\n",
    "print(new_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9602d62",
   "metadata": {},
   "source": [
    "### 5. Question Answering to Select Characters/Locations for Special Places\n",
    "#### 5.1 Get Wikipedia Data as Q&A Basis Data\n",
    "https://pypi.org/project/Wikipedia-API/0.3.5/\n",
    "- *To Do* : Select only \"Plot\" Section from Wikipedia Data/Find a way to get relevant data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afac8981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic is ok.\n"
     ]
    }
   ],
   "source": [
    "## for regular text output\n",
    "wiki_en_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "\n",
    "## check if page for topic exists\n",
    "if wiki_en_wiki.page(topic).exists():\n",
    "    print(\"Topic is ok.\")\n",
    "    wiki_page = wiki_en_wiki.page(topic)\n",
    "    topic_text = wiki_page.text\n",
    "else:\n",
    "    print(\"Find a new topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1834482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "q_a = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7f574",
   "metadata": {},
   "source": [
    "#### 5.2 Select Questions for Q&A model to get wikipedia responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4238191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare questions (examples, to discuss)\n",
    "question_dict = {\"special_1\": \"What is an important monument in the movie \",\n",
    "                \"special_2\": \"What is an expensive location in the movie \",\n",
    "                \"prison\": \"Which one is a tragic area in the movie \",\n",
    "                \"free_parking\": \"What is the loveliest place in the movie \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62e497c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is an important monument in the movie Furious 7?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:705: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      "/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:295: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  p_mask = np.asarray(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9653973579406738, 'start': 30306, 'end': 30312, 'answer': 'Walker'}\n",
      "What is an expensive location in the movie Furious 7?\n",
      "{'score': 0.6865812540054321, 'start': 4746, 'end': 4757, 'answer': 'Los Angeles'}\n",
      "Which one is a tragic area in the movie Furious 7?\n",
      "{'score': 0.7995515465736389, 'start': 29403, 'end': 29409, 'answer': 'Walker'}\n",
      "What is the loveliest place in the movie Furious 7?\n",
      "{'score': 0.46883273124694824, 'start': 13357, 'end': 13366, 'answer': 'Abu Dhabi'}\n"
     ]
    }
   ],
   "source": [
    "for category, question_body in question_dict.items():\n",
    "    question = question_body + topic + \"?\"\n",
    "    print(question)\n",
    "    \n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': topic_text\n",
    "    }\n",
    "    response = q_a(QA_input)\n",
    "    print(response)\n",
    "    \n",
    "    if category == \"special_1\":\n",
    "        new_field[\"special\"][\"1\"] = [response[\"answer\"]]\n",
    "    elif category == \"special_2\":\n",
    "        new_field[\"special\"][\"2\"] = [response[\"answer\"]]\n",
    "    else:\n",
    "        new_field[category] = [response[\"answer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bf1ea",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluate Responses:\n",
    "- To Do: Check if location/character already exists in new_field\n",
    "- To Do: filter for bad scores, retrigger question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97a57ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'streets': {'1-3': ['Mr. Nobody Street', 'Deckard Shaw Drive', 'Han Drive'],\n",
       "  '4-6': ['Sean Boswell Lane', 'Elena Boulevard', 'Hector Road'],\n",
       "  '7-9': ['Owen Shaw Drive', 'Safar Road', 'Jack Lane'],\n",
       "  '10-12': ['Samantha Hobbs Alley', 'Letty Fan Park', 'Female Racer Park'],\n",
       "  '13-15': ['Race Starter Lane', 'Hot Teacher Drive', 'Doctor Park'],\n",
       "  '16-18': ['Merc Tech Road', 'Weapons Tech Lane'],\n",
       "  'expensive': ['Dominic Toretto Avenue', \"Brian O'Conner Avenue\"],\n",
       "  'cheap': ['Kiet Drive', 'Kara Drive']},\n",
       " 'stations': ['Letty Station',\n",
       "  'Roman Station',\n",
       "  \"Tej (as Chris 'Ludacris' Bridges) Station\",\n",
       "  'Mia Station'],\n",
       " 'prison': ['Walker'],\n",
       " 'free_parking': ['Abu Dhabi'],\n",
       " 'special': {'1': ['Walker'], '2': ['Los Angeles']}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2785d",
   "metadata": {},
   "source": [
    "# 2. Generation of Action Cards "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975966cd",
   "metadata": {},
   "source": [
    "### 1. Plagiarism: Read in Monopoly Data\n",
    "- Get action verbs from monopoly data\n",
    "- use real action cards for few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a004b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_MONOPOLY = \"monopoly_action_cards_keywords.csv\"\n",
    "monopoly_data = pd.read_csv(FILENAME_MONOPOLY, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0dc816",
   "metadata": {},
   "source": [
    "#### 1.1 Keyword Preparation from Monopoly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "508f2caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic's keyword_list is:  ['Pay', 'take', 'come', '..', 'Go', 'get', '..', 'pays', 'are', 'pays', 'Go', 'receive', 'inherit', 'receive', '..', 'is', 'win', 'won', 'is', '..', 'receives', 'has', 'buy', 'get', 'have', 'been', 'elected', 'Have', 'renovated', 'Euro', 'be', 'called', 'do', 'Pay', 'Do', 'pass', 'collect', 'be', 'released', 'keep', 'need', 'sell', 'Do', 'pass', 'collect', 'be', 'released', 'keep', 'need', 'sell']\n"
     ]
    }
   ],
   "source": [
    "## get pos tags\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "## Get action verbs from real monopoly action cards\n",
    "text_data = \"\"\n",
    "for text_item in monopoly_data[\"content\"]:\n",
    "    text_data += \". \" + text_item\n",
    "    \n",
    "inspect_actions = preprocess(text_data)\n",
    "\n",
    "## use only verbs\n",
    "keyword_list_verbs = []\n",
    "\n",
    "for pos_tag in inspect_actions:\n",
    "    if re.match(\"VB.*\", pos_tag[1]):\n",
    "        if pos_tag[0] == \"DM\":\n",
    "            keyword_list_verbs.append(\"Euro\")\n",
    "        else:\n",
    "            keyword_list_verbs.append(pos_tag[0])  \n",
    "        \n",
    "print(\"The topic's keyword_list is: \", keyword_list_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f7bf7",
   "metadata": {},
   "source": [
    "Data Source Further Action Words:\n",
    "https://www.citationmachine.net/resources/grammar-guides/verb/list-verbs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e0b5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tag == \"VB.*\" from real monopoly action cards\n",
    "\n",
    "\n",
    "## belongs to DATA INIT\n",
    "action_verbs_monopoly = [\"Pay\",\"Take\",\"Come\",\"Go\",\"Get\",\"Receive\",\"Inherit\",\"Win\",\"Pass\",\n",
    "                         \"Collect\",\"being released\",\"Keep\",\"Sell\"]\n",
    "action_verbs = [\"Act\",\"Answer\",\"Approve\",\"Arrange\",\"Break\",\"Build\",\"Buy\",\"Coach\",\"Color\",\"Cough\",\"Create\", \n",
    "                \"Complete\",\"Cry\",\"Dance\",\"Describe\",\"Draw\",\"Drink\",\"Eat\",\"Edit\",\"Enter\",\"Exit\",\n",
    "                \"Imitate\",\"Invent\",\"Jump\",\"Laugh\",\"Lie\",\"Listen\",\"Paint\",\"Plan\",\"Play\",\"Read\",\"Replace\",\n",
    "                \"Run\",\"Scream\",\"See\",\"Shop\",\"Shout\",\"Sing\",\"Skip\",\"Sleep\",\"Sneeze\",\"Solve\",\"Study\",\"Teach\",\n",
    "                \"Touch\",\"Turn\",\"Walk\",\"Win\",\"Write\",\"Whistle\",\"Yank\",\"Zip\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b84b5727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO', 'Mr. Nobody Street', 'Deckard Shaw Drive', 'Han Drive', 'Sean Boswell Lane', 'Elena Boulevard', 'Hector Road', 'Owen Shaw Drive', 'Safar Road', 'Jack Lane', 'Samantha Hobbs Alley', 'Letty Fan Park', 'Female Racer Park', 'Race Starter Lane', 'Hot Teacher Drive', 'Doctor Park', 'Merc Tech Road', 'Weapons Tech Lane', 'Dominic Toretto Avenue', \"Brian O'Conner Avenue\", 'Kiet Drive', 'Kara Drive', 'Walker', 'Los Angeles', 'Letty Station', 'Roman Station', \"Tej (as Chris 'Ludacris' Bridges) Station\", 'Mia Station', 'Walker', 'Abu Dhabi']\n"
     ]
    }
   ],
   "source": [
    "## locations into flat list\n",
    "locations = [\"GO\"]\n",
    "for _, value in new_field[\"streets\"].items():\n",
    "    for item in value:\n",
    "        locations.append(item)\n",
    "for _, value in new_field[\"special\"].items():\n",
    "    for item in value:\n",
    "        locations.append(item)\n",
    "for item in new_field[\"stations\"]:\n",
    "    locations.append(item)\n",
    "locations.append(new_field[\"prison\"][0])\n",
    "locations.append(new_field[\"free_parking\"][0])    \n",
    "\n",
    "print(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e84d3",
   "metadata": {},
   "source": [
    "#### 1.2 Few-Shot Learning Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ab5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate few shot training data for text generation\n",
    "prompt_text = \"\"\n",
    "\n",
    "for text, keywords in zip(monopoly_data[\"content\"], monopoly_data[\"keywords\"]):\n",
    "    imd = \"key: \" + keywords + \"\\ntweet: \" + text + \"\\n###\"\n",
    "    prompt_text += imd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248656f",
   "metadata": {},
   "source": [
    "### 2. Random Keyword Generation for New Action Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "## global\n",
    "API_TOKEN = \"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\"\n",
    "## self prison\n",
    "\n",
    "## belongs to DATA INIT\n",
    "def prepare_generation_prompt(monopoly_data):\n",
    "    \n",
    "    generation_prompt_text = \"\"\n",
    "\n",
    "    for text, keywords in zip(monopoly_data[\"content\"], monopoly_data[\"keywords\"]):\n",
    "        imd = \"key: \" + keywords + \"\\ntweet: \" + text + \"\\n###\"\n",
    "        generation_prompt_text += imd\n",
    "    \n",
    "    return generation_prompt_text\n",
    "\n",
    "def prepare_sentiment_prompt(monopoly_data):\n",
    "    \n",
    "    sent_prompt_text = \"\"\n",
    "\n",
    "    for text, sentiment in zip(monopoly_data[\"content\"], monopoly_data[\"bias\"]):\n",
    "        imd = \"Tweet: \" + text + \"\\nSentiment: \" + sentiment + \"\\n###\"\n",
    "        sent_prompt_text += imd\n",
    "    \n",
    "    return sent_prompt_text\n",
    "\n",
    "def keyword_generation():\n",
    "\n",
    "    first_verb = random.choice(action_verbs_monopoly).lower()\n",
    "    second_verb = random.choice(action_verbs).lower()\n",
    "    pronoun = random.choice(pronouns).lower()\n",
    "    location = random.choice(locations)\n",
    "    number = 2000\n",
    "    select_number = random.choice([0,1])\n",
    "\n",
    "    ## create keyword list for generation\n",
    "    if select_number == 1:\n",
    "        keyword_list = [first_verb, second_verb, pronoun, location, number]\n",
    "    else:\n",
    "        keyword_list = [first_verb, second_verb, pronoun, location]\n",
    "    keyword_string = helper_keywords(keyword_list)\n",
    "    \n",
    "    return keyword_string\n",
    "\n",
    "\n",
    "def helper_keywords(keyword_list):\n",
    "    \n",
    "    keyword_string = \"\"\n",
    "\n",
    "    for item in keyword_list:\n",
    "        if keyword_string == \"\":\n",
    "            keyword_string += str(item)\n",
    "        else:\n",
    "            keyword_string += \", \" + str(item)\n",
    "            \n",
    "    return keyword_string\n",
    "\n",
    "\n",
    "def query(payload='',\n",
    "          parameters={'max_new_tokens': 25, 'temperature': 1, 'end_sequence': \"###\"},\n",
    "          options={'use_cache': False}):\n",
    "    \n",
    "    API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    body = {\"inputs\":payload,'parameters':parameters,'options':options}\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data= json.dumps(body))\n",
    "    try:\n",
    "      response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        return \"Error:\"+\" \".join(response.json()['error'])\n",
    "    else:\n",
    "      return response.json()[0]['generated_text']\n",
    "\n",
    "\n",
    "def generate_action_cards(action_verbs_monopoly, ## self\n",
    "                          action_verbs, ## self\n",
    "                          pronouns=[\"you\",\"your\",\"yours\"], ## self\n",
    "                          locations, ## self\n",
    "                          generation_prompt_text, ## self\n",
    "                          sent_prompt_text, ## self\n",
    "                          counter=0):\n",
    "    \n",
    "    if counter == 0:\n",
    "        action_cards = []\n",
    "        counter += 1\n",
    "        generate_action_cards(counter)\n",
    "        \n",
    "    ## stop generation of action cards\n",
    "    elif counter > 2:\n",
    "        return action_cards\n",
    "    \n",
    "    else:\n",
    "        ## keyword generation\n",
    "        keyword_string = keyword_generation()\n",
    "        ## action card generation\n",
    "        generation_prompt = generation_prompt_text + \"\\nkey: \" + keyword_string + \"\\ntweet:\"\n",
    "        data = query(generation_prompt)\n",
    "        action_card = re.findall(r\"(?<=tweet:\\s).*\", data)[-1] \n",
    "        \n",
    "        ## sentiment classification for action card\n",
    "        sent_prompt = sent_prompt_text + \"\\nTweet: \" + action_card + \"\\nSentiment:\"\n",
    "        sentiment = query(sent_prompt)\n",
    "        action_sentiment = re.findall(r\"(?<=Sentiment:\\s).*\", sentiment)[-1]\n",
    "        \n",
    "        ## get reference\n",
    "        reference = get_reference_for_sentence(location, action_card, action_sentiment)\n",
    "        \n",
    "        ## evaluate action card against reference\n",
    "        score = eval_sentence(reference, action_card)\n",
    "        \n",
    "        ## if action card OK, append\n",
    "        if score >= 0.5:\n",
    "            actions_cards.append((action_card, action_sentiment))\n",
    "            counter += 1\n",
    "            generate_action_cards(counter)\n",
    "        else:\n",
    "            counter += 1\n",
    "            generate_action_cards(counter)\n",
    "            \n",
    "        \n",
    "                 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94f1e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first_verb: go \n",
      "second_verb: yank \n",
      "pronoun: you \n",
      "LOCATION: Dominic Toretto Avenue \n",
      "number: 2000\n",
      "\n",
      "select_second_verb: 1 \n",
      "select_pronoun: 1 \n",
      "select_location: 1\n",
      "go, yank, you, Dominic Toretto Avenue\n"
     ]
    }
   ],
   "source": [
    "## once locations available, randomly select location\n",
    "\n",
    "\n",
    "## randomly select verbs, pronouns, locations, fixed number\n",
    "first_verb = random.choice(action_verbs_monopoly).lower()\n",
    "second_verb = random.choice(action_verbs).lower()\n",
    "pronoun = random.choice([\"you\",\"your\",\"yours\"]).lower()\n",
    "LOCATION = random.choice(locations)\n",
    "LOCATION = \"Dominic Toretto Avenue\"\n",
    "number = 2000\n",
    "\n",
    "## special case for prison\n",
    "if LOCATION == new_field[\"prison\"]:\n",
    "    keyword_list = [LOCATION, \"not pass\", \"not collect\"]\n",
    "\n",
    "else:\n",
    "    ## randomly select if second verb, pronoun and location should be considered\n",
    "    select_second_verb = 1\n",
    "    select_pronoun = 1\n",
    "    select_location = 1\n",
    "    #select_second_verb = random.choice([0,1])\n",
    "    #select_pronoun = random.choice([0,1])\n",
    "    #select_location = random.choice([0,1])\n",
    "\n",
    "    print(\"\\nfirst_verb:\", first_verb, \"\\nsecond_verb:\", second_verb, \"\\npronoun:\",  \\\n",
    "          pronoun, \"\\nLOCATION:\",LOCATION, \"\\nnumber:\",number )\n",
    "\n",
    "    print(\"\\nselect_second_verb:\", select_second_verb, \"\\nselect_pronoun:\", select_pronoun, \\\n",
    "          \"\\nselect_location:\",  select_location,)\n",
    "\n",
    "    if select_second_verb and select_pronoun and select_location:\n",
    "        keyword_list = [first_verb, second_verb, pronoun, LOCATION]\n",
    "    elif select_second_verb == 0 and select_pronoun and select_location:\n",
    "        keyword_list = [first_verb, pronoun, LOCATION]\n",
    "    elif select_second_verb == 0 and select_pronoun == 0 and select_location:\n",
    "        keyword_list = [first_verb, LOCATION]\n",
    "    elif select_second_verb == 0 and select_pronoun == 1 and select_location == 0:\n",
    "        keyword_list = [first_verb, pronoun, number]\n",
    "    elif select_second_verb == 0 and select_pronoun == 0 and select_location == 1:\n",
    "        keyword_list = [first_verb, LOCATION]\n",
    "    elif select_second_verb == 1 and select_pronoun == 0 and select_location == 0:\n",
    "        keyword_list = [first_verb, second_verb]\n",
    "    elif select_second_verb == 1 and select_pronoun == 0 and select_location == 1:\n",
    "        keyword_list = [first_verb, second_verb, LOCATION]\n",
    "    elif select_second_verb == 1 and select_pronoun == 1 and select_location == 0:\n",
    "        keyword_list = [first_verb, second_verb, pronoun]\n",
    "    elif select_second_verb == 0 and select_pronoun == 0 and select_location == 0:\n",
    "        keyword_list = [first_verb, number]\n",
    "\n",
    "\n",
    "keyword_string = \"\"\n",
    "\n",
    "for item in keyword_list:\n",
    "    if keyword_string == \"\":\n",
    "        keyword_string += str(item)\n",
    "    else:\n",
    "        keyword_string += \", \" + str(item)\n",
    "        \n",
    "print(keyword_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a51d34",
   "metadata": {},
   "source": [
    "### 2. Few-Shot Learning Key-to-Text Generation for Action Cards\n",
    "Source Inference API: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49ae406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## api token can be generated via free huggingface account\n",
    "API_TOKEN = \"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\"\n",
    "\n",
    "def query(payload='',\n",
    "          parameters={'max_new_tokens': 25, 'temperature': 1, 'end_sequence': \"###\"},\n",
    "          options={'use_cache': False}):\n",
    "    \n",
    "    API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    body = {\"inputs\":payload,'parameters':parameters,'options':options}\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data= json.dumps(body))\n",
    "    try:\n",
    "      response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        return \"Error:\"+\" \".join(response.json()['error'])\n",
    "    else:\n",
    "      return response.json()[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2376e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go, yank, you, Dominic Toretto Avenue\n",
      "Go into Dominic Toretto from the east. \n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens':25,  # number of generated tokens\n",
    "    'temperature': 1,   # controlling the randomness of generations\n",
    "    'end_sequence': \"###\" # stopping sequence for generation\n",
    "}\n",
    "\n",
    "prompt = prompt_text + \"\\nkey: \" + keyword_string + \"\\ntweet:\"\n",
    "\n",
    "\n",
    "data = query(prompt,parameters)\n",
    "\n",
    "action_card = re.findall(r\"(?<=tweet:\\s).*\", data)[-1] \n",
    "#print(data)\n",
    "print(keyword_string)\n",
    "print(action_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ca666",
   "metadata": {},
   "source": [
    "### 3. Evaluation of Generated Action Card\n",
    "\n",
    "- To Do: Which action cards should be used as reference for which input tokens?\n",
    "- To Do: Regularization List Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aadcf148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_distribution(pos_tuples_of_sentence):\n",
    "    \"\"\"\n",
    "    :pos_tuple_of_sentences: tuple (token, pos_tag) as returned from preprocess function\n",
    "    \n",
    "    crop pos tags into relevant groups (first two letters)\n",
    "    count occurences of pos tags in input sentence\n",
    "    \n",
    "    :returns: pandas DataFrame with pos_tag and its frequency\n",
    "    \n",
    "    \"\"\"\n",
    "    pos_df = pd.DataFrame(pos_tuples_of_sentence,columns=[\"token\",\"long_pos_tag\"])\n",
    "    pos_df[\"pos_tag\"] = [x[0:2] for x in pos_df[\"long_pos_tag\"]]\n",
    "    freq_df = pos_df[\"pos_tag\"].value_counts()\n",
    "    \n",
    "    return freq_df\n",
    "\n",
    "def eval_compare_structure(reference, new_sentence):\n",
    "\n",
    "    ## preprocess both\n",
    "    reference = preprocess(reference)\n",
    "    new_sentence = preprocess(new_sentence)\n",
    "    \n",
    "    ## pos distribution\n",
    "    reference = pos_distribution(reference)\n",
    "    new_sentence = pos_distribution(new_sentence)\n",
    "    \n",
    "    ## merge vectors\n",
    "    merged_df = pd.merge(reference,new_sentence,how=\"outer\", left_index=True,right_index=True).fillna(0)\n",
    "    merged_df.columns=[\"reference\",\"target\"]\n",
    "    \n",
    "    ## calc cosine similarity \n",
    "    similarity_score = 1 - spatial.distance.cosine(merged_df[\"reference\"], merged_df[\"target\"])\n",
    "    \n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dce0591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_compare_lengths(reference, new_sentence):\n",
    "    \n",
    "    reference = reference.split(\" \")\n",
    "    new_sentence = new_sentence.split(\" \")\n",
    "    \n",
    "    len_ref = len(reference)\n",
    "    len_new = len(new_sentence)\n",
    "    \n",
    "    if len_ref >= len_new:\n",
    "        len_score = len_new/len_ref\n",
    "    else:\n",
    "        len_score = len_ref/len_new\n",
    "    \n",
    "    return len_score\n",
    "\n",
    "def score_weighting(similarity_score, len_score, alpha = 0.3):\n",
    "    \n",
    "    score = alpha * len_score + (1-alpha) * similarity_score\n",
    "    \n",
    "    return score\n",
    "\n",
    "def eval_sentence(reference, new_sentence):\n",
    "    \n",
    "    similarity_score = eval_compare_structure(reference, new_sentence)\n",
    "    len_score = eval_compare_lengths(reference, new_sentence)\n",
    "    \n",
    "    score = score_weighting(similarity_score, len_score)\n",
    "    \n",
    "    return score\n",
    "    \n",
    "\n",
    "def get_reference_for_sentence(location, new_sentence, sentiment): ## monopoly_data):\n",
    "    \n",
    "    if location == PRISON:\n",
    "        if sentiment == \"positiv\":\n",
    "            reference = \"You will be released from prison! You must keep this card until you need it or sell it.\"\n",
    "        else:\n",
    "            reference = \"Go to the prison! Go directly there. Do not pass Go. Do not collect DM 4000,-.\"\n",
    "    else:\n",
    "        \n",
    "        if sentiment == \"positiv\":\n",
    "            pos_data = monopoly_data['content'][monopoly_data['bias'] == \"positiv\"]\n",
    "            pos_data = list(pos_data)\n",
    "            reference = random.choice(pos_data)\n",
    "        elif sentiment == \"negativ\":\n",
    "            neg_data = monopoly_data['content'][monopoly_data['bias'] == \"negativ\"]\n",
    "            neg_data = list(neg_data)\n",
    "            reference = random.choice(neg_data)\n",
    "        elif sentiment == \"neutral\":\n",
    "            neu_data = monopoly_data['content'][monopoly_data['bias'] == \"neutral\"]\n",
    "            neu_data = list(neu_data)\n",
    "            reference = random.choice(neu_data)\n",
    "    \n",
    "    return reference\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2d92f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You inherit: DM 2000,-.\n"
     ]
    }
   ],
   "source": [
    "some_data = monopoly_data['content'][monopoly_data['bias'] == \"positiv\"]\n",
    "#type(some_data)\n",
    "some_data = list(some_data)\n",
    "type(some_data)\n",
    "print(random.choice(some_data))\n",
    "#test = some_data.sample\n",
    "#print(some_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "11e8af84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.sample of 7               The bank pays you a dividend DM 1000,-.\n",
       "8     Rent and bond interest are due. The bank pays ...\n",
       "10    You receive a 7% dividend on preferred shares....\n",
       "11                              You inherit: DM 2000,-.\n",
       "12              From stock sales you receive: DM 500,-.\n",
       "13           The annual annuity is due. Draw DM 2000,-.\n",
       "14    You win a crossword puzzle contest. Draw DM 20...\n",
       "15            Bank error in your favor. Draw DM 4000,-.\n",
       "16                    Income tax refund. Draw DM 400,-.\n",
       "17    You won the 2nd prize in a beauty contest. Dra...\n",
       "18    It is your birthday. Collect DM 1000,- from ea...\n",
       "29    You will be released from prison! You must kee...\n",
       "31    You will be released from prison! You must kee...\n",
       "Name: content, dtype: object>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03205848",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"Go back to Badstraße.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eff1c4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go into Dominic Toretto from the east. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_card = \"Go into Dominic Toretto Avenue from the east. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15134579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4837119062359573, 0.4767312946227962, 0.5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentence(reference, action_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1036bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = reference.split(\" \")\n",
    "action_card = action_card.split(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e587ccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go', 'back', 'to', 'Badstraße.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "785138e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0375fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_card = \"Go into Dominic Toretto from the east. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3f8095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate few shot training data for sentiment classification\n",
    "sent_prompt_text = \"\"\n",
    "\n",
    "for text, sentiment in zip(monopoly_data[\"content\"], monopoly_data[\"bias\"]):\n",
    "    imd = \"Tweet: \" + text + \"\\nSentiment: \" + sentiment + \"\\n###\"\n",
    "    sent_prompt_text += imd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a04fe9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this street. Go direct to the bank. Pass GO and collect DM 1000,-.\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens':25,  # number of generated tokens\n",
    "    'temperature': 1,   # controlling the randomness of generations\n",
    "    'end_sequence': \"###\" # stopping sequence for generation\n",
    "}\n",
    "\n",
    "sent_prompt = sent_prompt_text + \"\\nTweet: \" + action_card + \"\\nSentiment:\"\n",
    "\n",
    "\n",
    "sentiment = query(sent_prompt,parameters)\n",
    "\n",
    "#action_card = re.findall(r\"(?<=tweet:\\s).*\", data)[-1] \n",
    "#print(data)\n",
    "print(action_card)\n",
    "#print(action_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e61519a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Pay a fine of DM 200,- or take a community ticket.\n",
      "Sentiment: negativ\n",
      "###Tweet: Move up to Seestrasse. \n",
      "If you come over Go, collect DM 4000,-.\n",
      "Sentiment: neutral\n",
      "###Tweet: Go back 3 fields.\n",
      "Sentiment: neutral\n",
      "###Tweet: Go back to Badstraße.\n",
      "Sentiment: neutral\n",
      "###Tweet: Move forward to Schlossallee.\n",
      "Sentiment: neutral\n",
      "###Tweet: Move forward to Go.\n",
      "Sentiment: neutral\n",
      "###Tweet: Make a trip to the south station. When you get over Go, draw DM 4000,-.\n",
      "Sentiment: neutral\n",
      "###Tweet: The bank pays you a dividend DM 1000,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: Rent and bond interest are due. The bank pays you DM 3000,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: Move forward to Go.\n",
      "Sentiment: neutral\n",
      "###Tweet: You receive a 7% dividend on preferred shares. DM 900,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: You inherit: DM 2000,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: From stock sales you receive: DM 500,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: The annual annuity is due. Draw DM 2000,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: You win a crossword puzzle contest. Draw DM 2000,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: Bank error in your favor. Draw DM 4000,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: Income tax refund. Draw DM 400,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: You won the 2nd prize in a beauty contest. Draw DM 200,-.\n",
      "Sentiment: positiv\n",
      "###Tweet: It is your birthday. Collect DM 1000,- from each player.\n",
      "Sentiment: positiv\n",
      "###Tweet: Advance to the next station. The owner receives double the normal rent. If no player has a claim to this station yet, you can buy it from the bank.\n",
      "Sentiment: neutral\n",
      "###Tweet: Pay school fees: DM 3000,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: Doctor's fees. Pay: DM 1000,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: Advance to the opera square. If you get over Go, collect DM 4000,-.\n",
      "Sentiment: neutral\n",
      "###Tweet: Fine for speeding: DM 300,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: You have been elected to the board. Pay each player DM 1000,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: Have all your houses renovated! Pay to the bank. For each house DM 500,- For each hotel DM 2000,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: Pay to the hospital: DM 2000,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: You will be called upon to do road repair work. \n",
      "Pay for your houses and hotels.\n",
      "DM 800,- per house. \n",
      "DM 2300,- per hotel to the bank.\n",
      "Sentiment: negativ\n",
      "###Tweet: Go to the prison! Go directly there. Do not pass Go. Do not collect DM 4000,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: You will be released from prison! You must keep this card until you need it or sell it.\n",
      "Sentiment: positiv\n",
      "###Tweet: Go to the prison! Go directly there. Do not pass Go. Do not collect DM 4000,-.\n",
      "Sentiment: negativ\n",
      "###Tweet: You will be released from prison! You must keep this card until you need it or sell it.\n",
      "Sentiment: positiv\n",
      "###\n",
      "Tweet: Go to this street. Go direct to the bank. Pass GO and collect DM 1000,-.\n",
      "Sentiment: negativ\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb26a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sentiment = re.findall(r\"(?<=Sentiment:\\s).*\", sentiment)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "786cf7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negativ\n",
      "Go to this street. Go direct to the bank. Pass GO and collect DM 1000,-.\n"
     ]
    }
   ],
   "source": [
    "print(action_sentiment)\n",
    "print(action_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7ab3bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO\n"
     ]
    }
   ],
   "source": [
    "## go to location\n",
    "for location in locations:\n",
    "    if location in action_card:\n",
    "        go_to_location = location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e43ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "m = re.search('[0-9]+', action_card)\n",
    "print(m.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35e58213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "action_card_2 = \"There is 1000 number, what is the return value\"\n",
    "m = re.search('[0-9]+', action_card_2)\n",
    "\n",
    "try: m.group(0)\n",
    "except AttributeError:\n",
    "    number = None\n",
    "else:\n",
    "    number = m.group(0)\n",
    "    \n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef0abf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_from_action_card(action_card):\n",
    "    \n",
    "    ## search action card for locations\n",
    "    for location in locations:\n",
    "        if location in action_card:\n",
    "            go_to_location = location\n",
    "    \n",
    "    ## extract number, if existing\n",
    "    m = re.search('[0-9]+', action_card)\n",
    "    try: m.group(0)    \n",
    "    except AttributeError:\n",
    "        number = None\n",
    "    else:\n",
    "        number = m.group(0)\n",
    "        \n",
    "        \n",
    "    return go_to_location, number\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32877771",
   "metadata": {},
   "outputs": [],
   "source": [
    "## action card structure\n",
    "##(action_card, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c41077a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GO', '1000')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_from_action_card(action_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3dc612b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Nobody Street\n",
      "Deckard Shaw Drive\n",
      "Han Drive\n",
      "Sean Boswell Lane\n",
      "Elena Boulevard\n",
      "Hector Road\n",
      "Owen Shaw Drive\n",
      "Safar Road\n",
      "Jack Lane\n",
      "Samantha Hobbs Alley\n",
      "Letty Fan Park\n",
      "Female Racer Park\n",
      "Race Starter Lane\n",
      "Hot Teacher Drive\n",
      "Doctor Park\n",
      "Merc Tech Road\n",
      "Weapons Tech Lane\n",
      "Dominic Toretto Avenue\n",
      "Brian O'Conner Avenue\n",
      "Kiet Drive\n",
      "Kara Drive\n",
      "Walker\n",
      "Los Angeles\n",
      "Letty Station\n",
      "Roman Station\n",
      "Tej (as Chris 'Ludacris' Bridges) Station\n",
      "Mia Station\n",
      "Walker\n",
      "Abu Dhabi\n"
     ]
    }
   ],
   "source": [
    "for location in locations:\n",
    "    print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7af26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_card = \"Go to this street. Go direct to the bank. Pass GO and collect DM 1000,-.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c9e634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.623829017247149, 0.7768985960673559, 0.26666666666666666)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentence(reference, action_card)\n",
    "#print(action_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01ad1ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this street. Go direct to the bank. Pass GO and collect DM 1000,-.\n"
     ]
    }
   ],
   "source": [
    "print(action_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a70676af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go back to Badstraße.\n"
     ]
    }
   ],
   "source": [
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fd49b7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GO',\n",
       " 'Mr. Nobody Street',\n",
       " 'Deckard Shaw Drive',\n",
       " 'Han Drive',\n",
       " 'Sean Boswell Lane',\n",
       " 'Elena Boulevard',\n",
       " 'Hector Road',\n",
       " 'Owen Shaw Drive',\n",
       " 'Safar Road',\n",
       " 'Jack Lane',\n",
       " 'Samantha Hobbs Alley',\n",
       " 'Letty Fan Park',\n",
       " 'Female Racer Park',\n",
       " 'Race Starter Lane',\n",
       " 'Hot Teacher Drive',\n",
       " 'Doctor Park',\n",
       " 'Merc Tech Road',\n",
       " 'Weapons Tech Lane',\n",
       " 'Dominic Toretto Avenue',\n",
       " \"Brian O'Conner Avenue\",\n",
       " 'Kiet Drive',\n",
       " 'Kara Drive',\n",
       " 'Walker',\n",
       " 'Los Angeles',\n",
       " 'Letty Station',\n",
       " 'Roman Station',\n",
       " \"Tej (as Chris 'Ludacris' Bridges) Station\",\n",
       " 'Mia Station',\n",
       " 'Walker',\n",
       " 'Abu Dhabi']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8f0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
