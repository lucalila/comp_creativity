{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58dbd955",
   "metadata": {},
   "source": [
    "# Computational Creativity Seminar, LMU, WS 2021/22\n",
    "\n",
    "## Project: Interdimensional Monopoly\n",
    "\n",
    "### Creators: Laura Luckert & Shaoqiu Zhang\n",
    "\n",
    "Topic Description: Zwei oder mehr Spieler spielen das klassische Monopoly, aber nach jedem \"über Los\" ändert sich das Thema des Spiels. Aus dem klassischen Monopoly wird ein \"Star Wars\" Monopoly, ein \"Herr der Ringe\" Monopoly etc. Die Namen der Straßen und Aktionsfelder ändern sich nach dem aktuellen Thema. Die Aktionskarten, Felder und Namen müssen sinnvoll generiert werden. Programmier-Aufwand sollte sich nicht auf eine aufwändige GUI richten, sondern auf die sinnvolle Generation neuer Dimensionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae06cc",
   "metadata": {},
   "source": [
    "#### Target:\n",
    "* Each new dimension is related to a popular Netflix movie / series\n",
    "* Within the dimension, places and actions are generated from places that exist in this movie/series and useful actions related to the respective movie/series\n",
    "\n",
    "#### Data Sources:\n",
    "* Kaggle series / movie dataset with user rankings: https://www.kaggle.com/chasewillden/netflix-shows\n",
    "* Fictional Worlds: https://github.com/prosecconetwork/The-NOC-List/blob/master/NOC/DATA/Veale's%20NOC%20List/Veales%20place%20elements.xlsx\n",
    "* Wikipedia API via https://pypi.org/project/Wikipedia-API/0.3.5/ \n",
    "* Regularization via https://hatebase.org and https://github.com/dariusk/wordfilter\n",
    "\n",
    "#### How To\n",
    "\n",
    "##### Randomization\n",
    "* Random selection of topic: Movie or series from netflix dataset, we only consider titles with a rating > 90 to pick only the most popular shows\n",
    "* For each topic, we retrieve the Wikipedia article, if there is none, the topic is discarded\n",
    "* Filtration: if Wikipedia article is too short, the topic is discarded\n",
    "\n",
    "##### Plagiarism\n",
    "* we use the regular Monopoly action cards in combination with the Wikipedia data for action card text generation\n",
    "\n",
    "##### Generation\n",
    "For places:\n",
    "* NER of wikipedia text to extract persons and locations\n",
    "\n",
    "For actions:\n",
    "* KeywordToText Generation with action card input and frequently used terms in the Wikipedia article (considering NERs)\n",
    "\n",
    "##### Filtration & Creation\n",
    "* Fitness: Find a fitness metric for the existing places and actions and compare the generated text against it\n",
    "* -> Self-evaluation of system -> keep only above a certain treshold, otherwise trigger re-generation\n",
    "\n",
    "#### Output Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651f209d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n\"topic\": \"Topic Name\",\\n\\n\"places\": {\\n    \"general_places\": [(\"Name of Place\", 1000), (\"Name of Place 2\", 2000)],\\n    \"train_stations\": [\"Place 1\", \"Place 2\", \"Place 3\", \"Place 4\"]\\n    \"jail\": \"Name of Place\",\\n    \"free_parking\": \"Name of Place\"\\n    },\\n    \\n\"actions\": {\\n    \"neutral_action\": [\"Go three fields back\", \"...\"],\\n    \"reward_action\": [\"Generate rewarding action for the player\", \"...\"],\\n    \"penalty_action\": [\"Generate punishing action for the player\", \"...\"]\\n    }\\n}\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{\n",
    "\"topic\": \"Topic Name\",\n",
    "\n",
    "\"places\": {\n",
    "    \"general_places\": [(\"Name of Place\", 1000), (\"Name of Place 2\", 2000)],\n",
    "    \"train_stations\": [(\"Place 1\",3000), (\"Place 2\",3000), (\"Place 3\",3000), (\"Place 4\",3000)]\n",
    "    \"jail\": \"Name of Place\",\n",
    "    \"free_parking\": \"Name of Place\"\n",
    "    },\n",
    "    \n",
    "\"actions\": {\n",
    "    \"neutral_action\": [\"Go three fields back\", \"...\"],\n",
    "    \"reward_action\": [\"Generate rewarding action for the player\", \"...\"],\n",
    "    \"penalty_action\": [\"Generate punishing action for the player\", \"...\"]\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b79d69",
   "metadata": {},
   "source": [
    "### 1. Select Topic (Dimension) via Netflix Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b77760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d3f68c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087b8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -n comp_creativity pandas -y\n",
    "PATH = \"~/Desktop/\"\n",
    "FILENAME = \"netflix_data.csv\"\n",
    "\n",
    "full_path = os.path.expanduser(PATH)\n",
    "os.chdir(full_path)\n",
    "\n",
    "netflix_data = pd.read_csv(FILENAME, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ce9821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>ratingLevel</th>\n",
       "      <th>ratingDescription</th>\n",
       "      <th>release year</th>\n",
       "      <th>user rating score</th>\n",
       "      <th>user rating size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White Chicks</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>crude and sexual humor, language and some drug...</td>\n",
       "      <td>80</td>\n",
       "      <td>2004</td>\n",
       "      <td>82.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lucky Number Slevin</td>\n",
       "      <td>R</td>\n",
       "      <td>strong violence, sexual content and adult lang...</td>\n",
       "      <td>100</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grey's Anatomy</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Parents strongly cautioned. May be unsuitable ...</td>\n",
       "      <td>90</td>\n",
       "      <td>2016</td>\n",
       "      <td>98.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prison Break</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Parents strongly cautioned. May be unsuitable ...</td>\n",
       "      <td>90</td>\n",
       "      <td>2008</td>\n",
       "      <td>98.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How I Met Your Mother</td>\n",
       "      <td>TV-PG</td>\n",
       "      <td>Parental guidance suggested. May not be suitab...</td>\n",
       "      <td>70</td>\n",
       "      <td>2014</td>\n",
       "      <td>94.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Supernatural</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Parents strongly cautioned. May be unsuitable ...</td>\n",
       "      <td>90</td>\n",
       "      <td>2016</td>\n",
       "      <td>95.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Breaking Bad</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>For mature audiences.  May not be suitable for...</td>\n",
       "      <td>110</td>\n",
       "      <td>2013</td>\n",
       "      <td>97.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Vampire Diaries</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Parents strongly cautioned. May be unsuitable ...</td>\n",
       "      <td>90</td>\n",
       "      <td>2017</td>\n",
       "      <td>91.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>For mature audiences.  May not be suitable for...</td>\n",
       "      <td>110</td>\n",
       "      <td>2015</td>\n",
       "      <td>98.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pretty Little Liars</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Parents strongly cautioned. May be unsuitable ...</td>\n",
       "      <td>90</td>\n",
       "      <td>2016</td>\n",
       "      <td>96.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title rating  \\\n",
       "0           White Chicks  PG-13   \n",
       "1    Lucky Number Slevin      R   \n",
       "2         Grey's Anatomy  TV-14   \n",
       "3           Prison Break  TV-14   \n",
       "4  How I Met Your Mother  TV-PG   \n",
       "5           Supernatural  TV-14   \n",
       "6           Breaking Bad  TV-MA   \n",
       "7    The Vampire Diaries  TV-14   \n",
       "8       The Walking Dead  TV-MA   \n",
       "9    Pretty Little Liars  TV-14   \n",
       "\n",
       "                                         ratingLevel  ratingDescription  \\\n",
       "0  crude and sexual humor, language and some drug...                 80   \n",
       "1  strong violence, sexual content and adult lang...                100   \n",
       "2  Parents strongly cautioned. May be unsuitable ...                 90   \n",
       "3  Parents strongly cautioned. May be unsuitable ...                 90   \n",
       "4  Parental guidance suggested. May not be suitab...                 70   \n",
       "5  Parents strongly cautioned. May be unsuitable ...                 90   \n",
       "6  For mature audiences.  May not be suitable for...                110   \n",
       "7  Parents strongly cautioned. May be unsuitable ...                 90   \n",
       "8  For mature audiences.  May not be suitable for...                110   \n",
       "9  Parents strongly cautioned. May be unsuitable ...                 90   \n",
       "\n",
       "   release year  user rating score  user rating size  \n",
       "0          2004               82.0                80  \n",
       "1          2006                NaN                82  \n",
       "2          2016               98.0                80  \n",
       "3          2008               98.0                80  \n",
       "4          2014               94.0                80  \n",
       "5          2016               95.0                80  \n",
       "6          2013               97.0                80  \n",
       "7          2017               91.0                80  \n",
       "8          2015               98.0                80  \n",
       "9          2016               96.0                80  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflix_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe609cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select only shows with rating score > 90\n",
    "netflix_subset = netflix_data[netflix_data[\"user rating score\"] > 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6d0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 271 titles remain\n",
    "netflix_subset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1240858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63    Criminal Minds\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## randomly select topic\n",
    "topic = netflix_subset.sample()[\"title\"]\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75c3f8",
   "metadata": {},
   "source": [
    "### 2. Get Data for Topic from Wikipedia API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e0fea",
   "metadata": {},
   "source": [
    "https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "6eb743c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic is ok.\n"
     ]
    }
   ],
   "source": [
    "## for regular text output\n",
    "wiki_en_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "\n",
    "## check if page for topic exists\n",
    "if wiki_en_wiki.page(topic).exists():\n",
    "    print(\"Topic is ok.\")\n",
    "    wiki_page = wiki_en_wiki.page(topic)\n",
    "else:\n",
    "    print(\"Find a new topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "052db0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Section: Plot (1):\n",
       "After defeating Owen Shaw and securing amnesty for their past crimes, Dom, Brian and the team have returned to the United States to live normal lives. Brian accustoms himself to life as a father, while Dom tries to help Letty Ortiz regain her memory. Meanwhile, Owen's older brother, Deckard, breaks into the hospital where the comatose Owen is held, before breaking into the DSS office in Los Angeles to extract profiles of Dom's crew. After revealing his identity, Deckard fights Luke Hobbs and escapes, detonating a bomb that severely injures Hobbs. Dom later learns from his sister Mia that she is pregnant again and convinces her to tell Brian. However, a letter bomb, sent from Tokyo, explodes and destroys the Toretto house shortly after Han Lue, a member of Dom's team, is apparently killed by Deckard in Tokyo. Dom travels to Tokyo to retrieve Han's body and acquires the objects found at the crash site from Han's friend, Sean Boswell.\n",
       "As Dom, Brian, Tej Parker, and Roman Pearce mourn Han and Gisele Yashar at Han's funeral in Los Angeles, Dom confronts Deckard in an underground tunnel. Deckard flees when a covert ops team arrives and opens fire, led by a man only known as Mr. Nobody. At his air base of operations in El Segundo, California, Mr. Nobody says that he will help Dom in stopping Deckard if he helps him get God's Eye, a computer program that uses digital devices to track down a person, and save its creator, a hacker named Ramsey, from a Nigerian terrorist named Mose Jakande.\n",
       "The team soon airdrops their off-road modified cars over the Caucasus Mountains in Azerbaijan, ambush Jakande's convoy and rescue Ramsey, and they go to Abu Dhabi and steal the flash drive containing the God's Eye chip from a billionaire hidden in a Lykan HyperSport. With God's Eye near telecommunications repeaters, Dom, Brian, Nobody, and his team track down and capture Deckard, but are ambushed by Jakande and his henchmen and forced to flee while Jakande obtains God's Eye, as the injured Mr. Nobody calls for medical attention. The team returns to Los Angeles where Dom plans to singlehandedly fight Deckard alone, while Letty, Brian, Tej, and Roman resolve to protect Ramsey from Jakande. Later on, Brian promises Mia that he will fully dedicate himself to their family after he defeats Deckard and Jakande.\n",
       "As Jakande pursues Brian and the rest of the team with a stealth helicopter and an aerial drone, Ramsey attempts to hack into God's Eye. Hobbs, discovering the situation, leaves the hospital and destroys the drone with an ambulance. Brian battles and kills Jakande's henchman Kiet before hijacking a signal repeater tower, allowing Ramsey to regain control of God's Eye and shut it down. Meanwhile, Dom and Deckard engage in a one-on-one brawl on top of a public parking garage. Jakande intervenes and attacks them both; Dom uses the distraction to defeat Deckard by causing part of the parking garage to collapse beneath him. Dom attempts to crash his Charger onto Jakande's helicopter; he leaves a bag of grenades on the helicopter and crashes on the rubble of the garage. Hobbs shoots the grenades, destroying the helicopter and killing Jakande. After Brian and Hobbs help Letty bring out Dom's unconscious body, she cradles him and fully regains her memories by remembering their wedding. Dom regains consciousness after Letty tells him this.\n",
       "Deckard is taken into custody by Hobbs and locked away in a secret, high-security CIA prison. The rest of the team relaxes on a tropical beach. Brian and Mia play with their son Jack while Dom, Letty, Roman, Tej, and Ramsey look on, acknowledging that Brian is happily retired with his family. Dom drives away and Brian catches up with him. As Dom recalls his memories with Brian, the two bid each other farewell and drive off in separate directions.\n",
       "Subsections (0):"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_page.sections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "b4e613ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*: Plot - After defeating Owen Shaw and securing a\n",
      "*: Cast - Vin Diesel as Dominic Toretto, a former \n",
      "*: Production - \n",
      "**: Development - On October 21, 2011, the Los Angeles Tim\n",
      "**: Filming - Principal photography began in early Sep\n",
      "**: Stunts - The \"airdrop\" sequence was conceived by \n",
      "**: Redevelopment of Walker's character - In January 2014, Time reported that Walk\n",
      "**: Music - The musical score was composed by Brian \n",
      "*: Release - \n",
      "**: Theatrical - The film originally scheduled to be rele\n",
      "**: Home media - Furious 7 was released on July 6, 2015 i\n",
      "*: Reception - \n",
      "**: Box office - Furious 7 grossed $353 million in the Un\n",
      "***: North America - Predictions for the opening weekend of F\n",
      "***: Outside North America - Furious 7 opened on April 1, 2015 in 12 \n",
      "**: Critical response - Furious 7 received positive reviews, wit\n",
      "**: Accolades - \n",
      "*: Sequel - A sequel, titled The Fate of the Furious\n",
      "*: See also - List of films featuring drones\n",
      "List of f\n",
      "*: Notes - \n",
      "*: References - DocumentsUniversal Pictures. \"Universal \n",
      "*: External links - Official website\n",
      "Furious 7 at IMDb\n",
      "Furio\n"
     ]
    }
   ],
   "source": [
    "def print_sections(sections, level=0):\n",
    "        for s in sections:\n",
    "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
    "                print_sections(s.sections, level + 1)\n",
    "print_sections(wiki_page.sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "eb73a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_text = wiki_page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34fbc25",
   "metadata": {},
   "source": [
    "### 3. NER on Data to Identify Places\n",
    "\n",
    "Code from: https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "https://medium.com/spatial-data-science/how-to-extract-locations-from-text-with-natural-language-processing-9b77035b3ea4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582322c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lauraluckert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lauraluckert/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b85c4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pos-tagging, maybe useful for actions?\n",
    "topic_text_pos = preprocess(topic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82134891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Vampire', 'NNP'),\n",
       " ('Diaries', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('American', 'JJ'),\n",
       " ('supernatural', 'NN'),\n",
       " ('teen', 'JJ'),\n",
       " ('drama', 'NN'),\n",
       " ('television', 'NN'),\n",
       " ('series', 'NN'),\n",
       " ('developed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Kevin', 'NNP'),\n",
       " ('Williamson', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Julie', 'NNP'),\n",
       " ('Plec', 'NNP'),\n",
       " (',', ','),\n",
       " ('based', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('series', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('same', 'JJ'),\n",
       " ('name', 'NN'),\n",
       " ('written', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('L.', 'NNP'),\n",
       " ('J.', 'NNP'),\n",
       " ('Smith', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('premiered', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('CW', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('September', 'NNP'),\n",
       " ('10', 'CD'),\n",
       " (',', ','),\n",
       " ('2009', 'CD'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('concluded', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('March', 'NNP'),\n",
       " ('10', 'CD'),\n",
       " (',', ','),\n",
       " ('2017', 'CD'),\n",
       " (',', ','),\n",
       " ('having', 'VBG'),\n",
       " ('aired', 'VBN'),\n",
       " ('171', 'CD'),\n",
       " ('episodes', 'NNS'),\n",
       " ('over', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " ('seasons', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('pilot', 'NN'),\n",
       " ('episode', 'NN'),\n",
       " ('attracted', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('largest', 'JJS'),\n",
       " ('audience', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('CW', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('premiere', 'NN'),\n",
       " ('since', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('network', 'NN'),\n",
       " ('began', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('2006', 'CD'),\n",
       " (';', ':'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('season', 'NN'),\n",
       " ('averaged', 'VBD'),\n",
       " ('3.60', 'CD'),\n",
       " ('million', 'CD'),\n",
       " ('viewers', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('most-watched', 'JJ'),\n",
       " ('series', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('network', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('supplanted', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Arrow', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('show', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('received', 'VBN'),\n",
       " ('numerous', 'JJ'),\n",
       " ('award', 'NN'),\n",
       " ('nominations', 'NNS'),\n",
       " (',', ','),\n",
       " ('winning', 'VBG'),\n",
       " ('four', 'CD'),\n",
       " ('People', 'NNPS'),\n",
       " (\"'s\", 'POS'),\n",
       " ('Choice', 'NNP'),\n",
       " ('Award', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('many', 'JJ'),\n",
       " ('Teen', 'NNP'),\n",
       " ('Choice', 'NNP'),\n",
       " ('Awards', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('April', 'NNP'),\n",
       " ('2015', 'CD'),\n",
       " (',', ','),\n",
       " ('lead', 'JJ'),\n",
       " ('actress', 'NN'),\n",
       " ('Nina', 'NNP'),\n",
       " ('Dobrev', 'NNP'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('played', 'VBD'),\n",
       " ('Elena', 'NNP'),\n",
       " ('Gilbert', 'NNP'),\n",
       " (',', ','),\n",
       " ('confirmed', 'VBD'),\n",
       " ('that', 'IN'),\n",
       " ('she', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('leaving', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('show', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('sixth', 'JJ'),\n",
       " ('season', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Dobrev', 'NNP'),\n",
       " ('returned', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('record', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('voiceover', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('seventh-season', 'JJ'),\n",
       " ('finale', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('returned', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('guest', 'NN'),\n",
       " ('star', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('finale', 'NN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('March', 'NNP'),\n",
       " ('2016', 'CD'),\n",
       " (',', ','),\n",
       " ('The', 'DT'),\n",
       " ('CW', 'NNP'),\n",
       " ('renewed', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('eighth', 'JJ'),\n",
       " ('season', 'NN'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('in', 'IN'),\n",
       " ('July', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('announced', 'VBD'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('eighth', 'JJ'),\n",
       " ('season', 'NN'),\n",
       " (',', ','),\n",
       " ('consisting', 'VBG'),\n",
       " ('of', 'IN'),\n",
       " ('16', 'CD'),\n",
       " ('episodes', 'NNS'),\n",
       " (',', ','),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('show', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('last', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('final', 'JJ'),\n",
       " ('season', 'NN'),\n",
       " ('began', 'VBD'),\n",
       " ('airing', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('October', 'NNP'),\n",
       " ('21', 'CD'),\n",
       " (',', ','),\n",
       " ('2016', 'CD'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('ended', 'VBD'),\n",
       " ('March', 'NNP'),\n",
       " ('10', 'CD'),\n",
       " (',', ','),\n",
       " ('2017', 'CD'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('concepts', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('characters', 'NNS'),\n",
       " ('developed', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('served', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('launch', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('media', 'NNS'),\n",
       " ('franchise', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('includes', 'VBZ'),\n",
       " ('other', 'JJ'),\n",
       " ('television', 'NN'),\n",
       " ('series', 'NN'),\n",
       " (',', ','),\n",
       " ('web', 'JJ'),\n",
       " ('series', 'NN'),\n",
       " (',', ','),\n",
       " ('novels', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('comic', 'JJ'),\n",
       " ('books', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('television', 'NN'),\n",
       " ('series', 'NN'),\n",
       " ('The', 'DT'),\n",
       " ('Originals', 'NNP'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('also', 'RB'),\n",
       " ('aired', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('CW', 'NNP'),\n",
       " (',', ','),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('major', 'JJ'),\n",
       " ('entry', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('collection', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('connected', 'VBN'),\n",
       " ('works', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Series', 'NNP'),\n",
       " ('overview', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('set', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('fictional', 'JJ'),\n",
       " ('town', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Mystic', 'NNP'),\n",
       " ('Falls', 'NNP'),\n",
       " (',', ','),\n",
       " ('Virginia', 'NNP'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('town', 'NN'),\n",
       " ('charged', 'VBN'),\n",
       " ('with', 'IN'),\n",
       " ('supernatural', 'JJ'),\n",
       " ('history', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('follows', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('life', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Elena', 'NNP'),\n",
       " ('Gilbert', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Nina', 'NNP'),\n",
       " ('Dobrev', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('teenage', 'JJ'),\n",
       " ('girl', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('has', 'VBZ'),\n",
       " ('just', 'RB'),\n",
       " ('lost', 'VBN'),\n",
       " ('both', 'DT'),\n",
       " ('parents', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('car', 'NN'),\n",
       " ('accident', 'NN'),\n",
       " (',', ','),\n",
       " ('as', 'IN'),\n",
       " ('she', 'PRP'),\n",
       " ('falls', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('love', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('162-year-old', 'JJ'),\n",
       " ('vampire', 'NN'),\n",
       " ('named', 'VBN'),\n",
       " ('Stefan', 'NNP'),\n",
       " ('Salvatore', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Paul', 'NNP'),\n",
       " ('Wesley', 'NNP'),\n",
       " (')', ')'),\n",
       " ('.', '.'),\n",
       " ('Their', 'PRP$'),\n",
       " ('relationship', 'NN'),\n",
       " ('becomes', 'VBZ'),\n",
       " ('increasingly', 'RB'),\n",
       " ('complicated', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('Stefan', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('mysterious', 'JJ'),\n",
       " ('older', 'JJR'),\n",
       " ('brother', 'NN'),\n",
       " ('Damon', 'NNP'),\n",
       " ('Salvatore', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Ian', 'NNP'),\n",
       " ('Somerhalder', 'NNP'),\n",
       " (')', ')'),\n",
       " ('returns', 'NNS'),\n",
       " (',', ','),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('plan', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('bring', 'VB'),\n",
       " ('back', 'RP'),\n",
       " ('their', 'PRP$'),\n",
       " ('past', 'JJ'),\n",
       " ('love', 'NN'),\n",
       " ('Katherine', 'NNP'),\n",
       " ('Pierce', 'NNP'),\n",
       " ('(', '('),\n",
       " ('also', 'RB'),\n",
       " ('played', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Dobrev', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('vampire', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('looks', 'VBZ'),\n",
       " ('exactly', 'RB'),\n",
       " ('like', 'JJ'),\n",
       " ('Elena', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Although', 'IN'),\n",
       " ('Damon', 'NNP'),\n",
       " ('initially', 'RB'),\n",
       " ('harbors', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('grudge', 'NN'),\n",
       " ('against', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('brother', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('forcing', 'VBG'),\n",
       " ('him', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('become', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('vampire', 'NN'),\n",
       " (',', ','),\n",
       " ('he', 'PRP'),\n",
       " ('later', 'RB'),\n",
       " ('reconciles', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('Stefan', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('falls', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('love', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('Elena', 'NNP'),\n",
       " (',', ','),\n",
       " ('creating', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('love', 'NN'),\n",
       " ('triangle', 'NN'),\n",
       " ('among', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('three', 'CD'),\n",
       " ('.', '.'),\n",
       " ('Both', 'DT'),\n",
       " ('brothers', 'NNS'),\n",
       " ('attempt', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('protect', 'VB'),\n",
       " ('Elena', 'NNP'),\n",
       " ('as', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('face', 'VBP'),\n",
       " ('various', 'JJ'),\n",
       " ('villains', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('threats', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('their', 'PRP$'),\n",
       " ('town', 'NN'),\n",
       " (',', ','),\n",
       " ('including', 'VBG'),\n",
       " ('Katherine', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('brothers', 'NNS'),\n",
       " (\"'\", 'POS'),\n",
       " ('history', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('town', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('mythology', 'NN'),\n",
       " ('are', 'VBP'),\n",
       " ('revealed', 'VBN'),\n",
       " ('through', 'IN'),\n",
       " ('flashbacks', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('goes', 'VBZ'),\n",
       " ('on', 'IN'),\n",
       " ('.', '.'),\n",
       " ('Additional', 'NNP'),\n",
       " ('storylines', 'NNS'),\n",
       " ('revolve', 'VBP'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('inhabitants', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('town', 'NN'),\n",
       " (',', ','),\n",
       " ('most', 'RBS'),\n",
       " ('notably', 'RB'),\n",
       " ('Elena', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('younger', 'JJR'),\n",
       " ('brother', 'NN'),\n",
       " ('Jeremy', 'NNP'),\n",
       " ('Gilbert', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Steven', 'NNP'),\n",
       " ('R.', 'NNP'),\n",
       " ('McQueen', 'NNP'),\n",
       " (')', ')'),\n",
       " ('and', 'CC'),\n",
       " ('aunt', '$'),\n",
       " ('Jenna', 'NNP'),\n",
       " ('Sommers', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Sara', 'NNP'),\n",
       " ('Canning', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('her', 'PRP$'),\n",
       " ('best', 'JJS'),\n",
       " ('friends', 'NNS'),\n",
       " ('Bonnie', 'NNP'),\n",
       " ('Bennett', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Kat', 'NNP'),\n",
       " ('Graham', 'NNP'),\n",
       " (')', ')'),\n",
       " ('and', 'CC'),\n",
       " ('Caroline', 'NNP'),\n",
       " ('Forbes', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Candice', 'NNP'),\n",
       " ('King', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('their', 'PRP$'),\n",
       " ('mutual', 'JJ'),\n",
       " ('friends', 'NNS'),\n",
       " ('Matt', 'NNP'),\n",
       " ('Donovan', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Zach', 'NNP'),\n",
       " ('Roerig', 'NNP'),\n",
       " (')', ')'),\n",
       " ('and', 'CC'),\n",
       " ('Tyler', 'NNP'),\n",
       " ('Lockwood', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Michael', 'NNP'),\n",
       " ('Trevino', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('Matt', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('older', 'JJR'),\n",
       " ('sister', 'NN'),\n",
       " ('Vicki', 'NNP'),\n",
       " ('Donovan', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Kayla', 'NNP'),\n",
       " ('Ewell', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('their', 'PRP$'),\n",
       " ('history', 'NN'),\n",
       " ('teacher', 'NN'),\n",
       " (',', ','),\n",
       " ('vampire', 'NN'),\n",
       " ('hunter', 'NN'),\n",
       " ('Alaric', 'NNP'),\n",
       " ('Saltzman', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Matthew', 'NNP'),\n",
       " ('Davis', 'NNP'),\n",
       " (')', ')'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('town', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('politics', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('orchestrated', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('descendants', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('original', 'JJ'),\n",
       " ('founding', 'NN'),\n",
       " ('families', 'NNS'),\n",
       " (',', ','),\n",
       " ('all', 'DT'),\n",
       " ('comprising', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('``', '``'),\n",
       " ('Founders', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('Council', 'NNP'),\n",
       " ('.', '.'),\n",
       " (\"''\", \"''\"),\n",
       " ('They', 'PRP'),\n",
       " ('guard', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('town', 'NN'),\n",
       " ('mainly', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('vampires', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('other', 'JJ'),\n",
       " ('supernatural', 'JJ'),\n",
       " ('threats', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('werewolves', 'NNS'),\n",
       " (',', ','),\n",
       " ('witches', 'NNS'),\n",
       " (',', ','),\n",
       " ('hybrids', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('ghosts', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Cast', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('characters', 'NNS'),\n",
       " ('Nina', 'NNP'),\n",
       " ('Dobrev', 'NNP'),\n",
       " ('as', 'IN'),\n",
       " ('Elena', 'NNP'),\n",
       " ('Gilbert', 'NNP'),\n",
       " ('(', '('),\n",
       " ('season', 'NN'),\n",
       " ('1–6', 'CD'),\n",
       " (';', ':'),\n",
       " ('guest', 'JJS'),\n",
       " ('season', 'NN'),\n",
       " ('8', 'CD'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('young', 'JJ'),\n",
       " ('orphan', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('falls', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('love', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('vampire', 'NN'),\n",
       " ('Stefan', 'NNP'),\n",
       " ('Salvatore', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('later', 'RB'),\n",
       " ('his', 'PRP$'),\n",
       " ('brother', 'NN'),\n",
       " (',', ','),\n",
       " ('Damon', 'NNP'),\n",
       " (',', ','),\n",
       " ('creating', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('love', 'NN'),\n",
       " ('triangle', 'NN'),\n",
       " ('.', '.'),\n",
       " ('When', 'WRB'),\n",
       " ('Stefan', 'NNP'),\n",
       " ('shuts', 'VBZ'),\n",
       " ('his', 'PRP$'),\n",
       " ('humanity', 'NN'),\n",
       " ('off', 'RB'),\n",
       " ('after', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('blackmailed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Klaus', 'NNP'),\n",
       " (',', ','),\n",
       " ('Damon', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('given', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('opportunity', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('grow', 'VB'),\n",
       " ('closer', 'JJR'),\n",
       " ('to', 'TO'),\n",
       " ('Elena', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('results', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('being', 'VBG'),\n",
       " ('further', 'JJ'),\n",
       " ('drawn', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('supernatural', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('results', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('struggling', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('survive', 'VB'),\n",
       " ('supernatural', 'JJ'),\n",
       " ('events', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('town', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Dobrev', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('portrays', 'VBZ'),\n",
       " ('Elena', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('doppelgänger', 'NN'),\n",
       " (',', ','),\n",
       " ('Katerina', 'NNP'),\n",
       " ('Petrova', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('known', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('Katherine', 'NNP'),\n",
       " ('Pierce', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Dobrev', 'NNP'),\n",
       " ('sporadically', 'RB'),\n",
       " ('appeared', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('subsequent', 'JJ'),\n",
       " ('seasons', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('played', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('significant', 'JJ'),\n",
       " ('role', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('fifth', 'NN'),\n",
       " ('season', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Dobrev', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('plays', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('progenitor', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Petrova', 'NNP'),\n",
       " ('doppelgängers', 'NNS'),\n",
       " (',', ','),\n",
       " ('Silas', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('true', 'JJ'),\n",
       " ('love', 'NN'),\n",
       " ('known', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('Amara', 'NNP'),\n",
       " (',', ','),\n",
       " ('during', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('fifth', 'NN'),\n",
       " ('season', 'NN'),\n",
       " (',', ','),\n",
       " ('whom', 'WP'),\n",
       " ('he', 'PRP'),\n",
       " ('had', 'VBD'),\n",
       " ('sought', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('afterlife', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('two', 'CD'),\n",
       " ('thousand', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Dobrev', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('played', 'VBD'),\n",
       " ('another', 'DT'),\n",
       " ('doppelgänger', 'NN'),\n",
       " ('Tatia', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('Originals', 'NNP'),\n",
       " ('season', 'NN'),\n",
       " ('2', 'CD'),\n",
       " ('episode', 'NN'),\n",
       " ('``', '``'),\n",
       " ('Red', 'JJ'),\n",
       " ('Door', 'NNP'),\n",
       " ('.', '.'),\n",
       " (\"''\", \"''\"),\n",
       " ('In', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('season', 'NN'),\n",
       " ('6', 'CD'),\n",
       " ('finale', 'NN'),\n",
       " (',', ','),\n",
       " ('Elena', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('life', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('tied', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('Bonnie', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('in', 'IN'),\n",
       " ('such', 'JJ'),\n",
       " ('a', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('as', 'RB'),\n",
       " ('long', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('Bonnie', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('alive', 'JJ'),\n",
       " (',', ','),\n",
       " ('Elena', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('remain', 'VB'),\n",
       " ('asleep', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Damon', 'NNP'),\n",
       " ('put', 'VBD'),\n",
       " ('Elena', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('coffin', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('had', 'VBD'),\n",
       " ('her', 'PRP$'),\n",
       " ('hidden', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('warehouse', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Brooklyn', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('next', 'JJ'),\n",
       " ('60', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('so', 'RB'),\n",
       " ('while', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('waits', 'VBZ'),\n",
       " ('for', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('to', 'TO'),\n",
       " ('wake', 'VB'),\n",
       " ('up', 'RP'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('finale', 'NN'),\n",
       " (',', ','),\n",
       " ('Elena', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('curse', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('broken', 'VBN'),\n",
       " (',', ','),\n",
       " ('she', 'PRP'),\n",
       " ('reunites', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('Damon', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('live', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('long', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('happy', 'JJ'),\n",
       " ('life', 'NN'),\n",
       " ('together', 'RB'),\n",
       " ('with', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('becoming', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('doctor', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Paul', 'NNP'),\n",
       " ('Wesley', 'NNP'),\n",
       " ('as', 'IN'),\n",
       " ('Stefan', 'NNP'),\n",
       " ('Salvatore', 'NNP'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('good-hearted', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('affectionate', 'JJ'),\n",
       " ('vampire', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('complete', 'JJ'),\n",
       " ('opposite', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('older', 'NN'),\n",
       " ('brother', 'NN'),\n",
       " (',', ','),\n",
       " ('Damon', 'NNP'),\n",
       " ('Salvatore', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Later', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('series', 'NN'),\n",
       " (',', ','),\n",
       " ('Stefan', 'NNP'),\n",
       " ('reverts', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('his', 'PRP$'),\n",
       " ('old', 'JJ'),\n",
       " ('ways', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('Ripper', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('save', 'VB'),\n",
       " ('Damon', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('werewolf', 'NN'),\n",
       " ('bite', 'NN'),\n",
       " ('.', '.'),\n",
       " ('His', 'PRP$'),\n",
       " ('role', 'NN'),\n",
       " ('becomes', 'VBZ'),\n",
       " ('more', 'RBR'),\n",
       " ('antagonistic', 'JJ'),\n",
       " (',', ','),\n",
       " ('after', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('forced', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('turn', 'VB'),\n",
       " ('his', 'PRP$'),\n",
       " ('humanity', 'NN'),\n",
       " ('off', 'RB'),\n",
       " ('.', '.'),\n",
       " ('Eventually', 'RB'),\n",
       " (',', ','),\n",
       " ('he', 'PRP'),\n",
       " ('returns', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('his', 'PRP$'),\n",
       " ('good-hearted', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('caring', 'VBG'),\n",
       " ('self', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('reconciles', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('Elena', 'NNP'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('relationship', 'NN'),\n",
       " ('does', 'VBZ'),\n",
       " (\"n't\", 'RB'),\n",
       " ('last', 'JJ'),\n",
       " ('long', 'RB'),\n",
       " ('.', '.'),\n",
       " ('He', 'PRP'),\n",
       " ('becomes', 'VBZ'),\n",
       " ('human', 'JJ'),\n",
       " ('again', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('marries', 'NNS'),\n",
       " ('Caroline', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Season', 'NNP'),\n",
       " ('8', 'CD'),\n",
       " ('and', 'CC'),\n",
       " ('is', 'VBZ'),\n",
       " ('killed', 'VBN'),\n",
       " ('afterwards', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('finale', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('sacrifices', 'VBZ'),\n",
       " ('himself', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('save', 'VB'),\n",
       " ('Mystic', 'NNP'),\n",
       " ('Falls', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Wesley', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('portrays', 'VBZ'),\n",
       " ('his', 'PRP$'),\n",
       " ('revealed', 'JJ'),\n",
       " ('doppelgänger', 'NN'),\n",
       " (',', ','),\n",
       " ('Silas', 'NNP'),\n",
       " ('(', '('),\n",
       " ('seasons', 'NNS'),\n",
       " ('4–5', 'CD'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('first', 'JJ'),\n",
       " ('immortal', 'JJ'),\n",
       " ('being', 'VBG'),\n",
       " ('.', '.'),\n",
       " ('Wesley', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_text_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c84c1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "ner_w = spacy.load('xx_ent_wiki_sm')\n",
    "## roberta based\n",
    "#nlp_b = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "9f17d10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fast & Furious 7', 'ORG'), ('2015', 'DATE'), ('American', 'NORP'), ('James Wan', 'PERSON'), ('Chris Morgan', 'PERSON'), ('Fast & Furious', 'ORG'), ('6', 'CARDINAL'), ('2013', 'DATE'), ('seventh', 'ORDINAL'), ('the Fast & Furious', 'ORG'), ('Vin Diesel', 'PERSON'), ('Paul Walker', 'PERSON'), ('Dwayne Johnson', 'PERSON'), ('Michelle Rodriguez', 'PERSON'), ('Tyrese Gibson', 'PERSON'), ('Chris', 'PERSON'), ('Bridges', 'PERSON'), ('Jordana Brewster', 'PERSON'), ('Djimon Hounsou', 'ORG'), ('Kurt Russell', 'PERSON'), ('Jason Statham', 'ORG'), ('Dominic Toretto', 'PERSON'), (\"Brian O'Conner\", 'PERSON'), ('the United States', 'GPE'), ('Deckard Shaw', 'PERSON'), (\"Paul Walker's\", 'PERSON'), (\"O'Connor\", 'PERSON'), ('November 30', 'DATE'), ('2013.Plans', 'CARDINAL'), ('seventh', 'ORDINAL'), ('first', 'ORDINAL'), ('February 2012', 'DATE'), ('Johnson', 'PERSON'), ('Fast & Furious', 'ORG'), ('6', 'CARDINAL'), ('April 2013', 'DATE'), ('Wan', 'PERSON'), ('Diesel', 'ORG'), ('that same month', 'DATE'), ('September', 'DATE'), ('Atlanta', 'GPE'), ('November', 'DATE'), ('Walker', 'ORG'), ('April 2014', 'DATE'), ('July', 'DATE'), ('Walkers', 'PERSON'), ('Caleb', 'PERSON'), ('Cody', 'PERSON'), ('2015', 'DATE'), ('Los Angeles', 'GPE'), ('Colorado', 'GPE'), ('Abu Dhabi', 'GPE'), ('Tokyo', 'GPE'), ('up to $250 million', 'MONEY'), ('7', 'CARDINAL'), ('Los Angeles', 'GPE'), ('April 1, 2015', 'DATE'), ('the United States', 'GPE'), ('April 3', 'DATE'), ('six years', 'DATE'), ('fourth', 'ORDINAL'), ('Walker', 'ORG'), ('$397.6 million', 'MONEY'), ('opening weekend', 'DATE'), ('$1.5 billion', 'MONEY'), ('third', 'ORDINAL'), ('2015', 'DATE'), ('fourth', 'ORDINAL'), ('2015', 'DATE'), ('the first twelve days', 'DATE'), ('The Fate of the Furious', 'WORK_OF_ART'), ('April 2017', 'DATE'), ('Owen Shaw', 'PERSON'), ('Brian', 'PERSON'), ('the United States', 'GPE'), ('Brian', 'PERSON'), ('Letty Ortiz', 'PERSON'), ('Owen', 'ORG'), ('Deckard', 'ORG'), ('Owen', 'PERSON'), ('DSS', 'ORG'), ('Los Angeles', 'GPE'), ('Dom', 'ORG'), ('Deckard', 'ORG'), ('Luke Hobbs', 'PERSON'), ('Hobbs', 'GPE'), ('Mia', 'PERSON'), ('Brian', 'PERSON'), ('Tokyo', 'GPE'), ('Toretto', 'NORP'), ('Han Lue', 'ORG'), ('Dom', 'ORG'), ('Deckard', 'GPE'), ('Tokyo', 'GPE'), ('Tokyo', 'GPE'), ('Han', 'NORP'), ('Han', 'NORP'), ('Sean Boswell', 'PERSON'), ('Tej Parker', 'PERSON'), ('Roman Pearce', 'PERSON'), ('Han', 'NORP'), ('Gisele Yashar', 'PERSON'), ('Han', 'NORP'), ('Los Angeles', 'GPE'), ('Nobody', 'PERSON'), ('El Segundo', 'GPE'), ('California', 'GPE'), ('Nobody', 'PERSON'), ('Eye', 'PRODUCT'), ('Ramsey', 'PERSON'), ('Nigerian', 'NORP'), ('Mose Jakande', 'PERSON'), ('the Caucasus Mountains', 'LAW'), ('Azerbaijan', 'GPE'), ('Jakande', 'ORG'), ('Ramsey', 'PERSON'), ('Abu Dhabi', 'ORG'), ('Eye', 'PRODUCT'), ('a Lykan HyperSport', 'LOC'), ('Deckard', 'ORG'), ('Eye', 'PRODUCT'), ('Nobody', 'PERSON'), ('Los Angeles', 'GPE'), ('Dom', 'ORG'), ('Deckard', 'ORG'), ('Letty, Brian, Tej', 'ORG'), ('Roman', 'PERSON'), ('Ramsey', 'PERSON'), ('Brian', 'PERSON'), ('Mia', 'PERSON'), ('Jakande', 'ORG'), ('Brian', 'PERSON'), ('Ramsey', 'PERSON'), ('Brian', 'PERSON'), ('Jakande', 'ORG'), ('Kiet', 'PERSON'), ('Ramsey', 'PERSON'), ('Eye', 'PRODUCT'), ('Dom', 'ORG'), ('one', 'CARDINAL'), ('Deckard', 'ORG'), ('Jakande', 'ORG'), ('Jakande', 'ORG'), ('Brian', 'PERSON'), ('Hobbs', 'GPE'), ('Letty', 'PERSON'), ('Hobbs', 'GPE'), ('CIA', 'ORG'), ('Brian', 'PERSON'), ('Mia', 'PERSON'), ('Jack', 'PERSON'), ('Ramsey', 'PERSON'), ('Brian', 'PERSON'), ('Brian', 'PERSON'), ('Dom', 'PERSON'), ('Brian', 'PERSON'), ('two', 'CARDINAL'), ('Vin Diesel', 'PERSON'), ('Dominic Toretto', 'PERSON'), ('Letty', 'PERSON'), ('Paul Walker', 'PERSON'), (\"Brian O'Conner\", 'PERSON'), ('FBI', 'ORG'), ('Mia', 'PERSON'), ('Jack', 'PERSON'), ('Walker', 'ORG'), ('Cody Walker', 'PERSON'), ('Dwayne Johnson', 'PERSON'), ('Luke Hobbs', 'PERSON'), ('DSS', 'ORG'), ('Rio de Janeiro', 'FAC'), ('Europe', 'LOC'), ('Johnson', 'PERSON'), ('Universal Pictures', 'ORG'), ('seventh', 'ORDINAL'), ('the summer', 'DATE'), ('Hercules', 'GPE'), ('2014', 'DATE'), ('September', 'DATE'), ('Johnson', 'PERSON'), ('Hercules', 'PERSON'), ('7th', 'ORDINAL'), ('Michelle Rodriguez', 'PERSON'), ('Letty Ortiz', 'PERSON'), ('Fast & Furious', 'ORG'), ('2009', 'DATE'), ('Tyrese Gibson', 'PERSON'), ('Roman Pearce', 'PERSON'), ('Brian', 'PERSON'), ('Dom', 'ORG'), ('Chris', 'PERSON'), ('Tej Parker', 'PERSON'), ('Miami', 'GPE'), ('Dom', 'ORG'), ('Jordana Brewster', 'PERSON'), ('Mia Toretto', 'PERSON'), ('Dom', 'ORG'), ('Brian', 'PERSON'), ('Jack', 'PERSON'), ('Djimon Hounsou', 'PERSON'), ('Mose Jakande', 'PERSON'), ('Nigerian', 'NORP'), ('Shaw', 'ORG'), ('Tony Jaa', 'PERSON'), ('Jakande', 'ORG'), ('Thai', 'NORP'), ('Jaa', 'PERSON'), ('August 2013', 'DATE'), ('Hollywood', 'GPE'), ('Ronda Rousey', 'PERSON'), ('Kara', 'PERSON'), ('Abu Dhabi', 'GPE'), ('Rousey', 'GPE'), ('August 2013', 'DATE'), ('The Expendables 3', 'WORK_OF_ART'), ('2014', 'DATE'), ('Rousey', 'PERSON'), ('45 days', 'DATE'), ('UFC', 'FAC'), ('Miesha Tate', 'PERSON'), ('Gina Carano', 'PERSON'), ('Carano', 'PERSON'), ('Fast & Furious', 'ORG'), ('6', 'CARDINAL'), ('Nathalie Emmanuel', 'PERSON'), ('Ramsey', 'PERSON'), ('British', 'NORP'), ('Eye', 'PRODUCT'), ('Jakande', 'ORG'), ('Kurt Russell', 'PERSON'), ('Nobody', 'PERSON'), ('Shaw', 'PERSON'), ('Jason Statham', 'PERSON'), ('Deckard Shaw', 'PERSON'), ('British', 'NORP'), ('Spain', 'GPE'), ('Lucas Black', 'GPE'), ('Sean Boswell', 'PERSON'), ('American', 'NORP'), ('Tokyo', 'GPE'), ('Tokyo', 'GPE'), ('Han', 'NORP'), ('Shaw', 'ORG'), ('September', 'DATE'), ('Boswell for Furious 7', 'ORG'), ('two', 'CARDINAL'), ('Elsa Pataky', 'PERSON'), ('DSS', 'ORG'), ('Rio', 'ORG'), ('the United States', 'GPE'), ('Hobbs', 'ORG'), ('DSS', 'ORG'), ('John Brotherton', 'PERSON'), ('Sheppard', 'ORG'), ('Nobody', 'PERSON'), ('Ali Fazal', 'PERSON'), ('Safar', 'ORG'), ('Ramsey', 'PERSON'), ('Fazal', 'PERSON'), ('Luke Evans', 'PERSON'), ('Owen Shaw', 'PERSON'), ('Deckard', 'ORG'), ('Noel Gugliemi', 'PERSON'), ('2001', 'DATE'), ('Sung Kang', 'PERSON'), ('Han Lue', 'ORG'), ('Fast & Furious', 'ORG'), ('6', 'CARDINAL'), ('Australian', 'NORP'), ('Iggy Azalea', 'PERSON'), ('American', 'NORP'), ('Abu Dhabi', 'GPE'), ('Romeo Santos', 'PERSON'), ('Mando', 'ORG'), ('Mia', 'PERSON'), ('the Dominican Republic', 'GPE'), ('Klement Tinaj', 'ORG'), ('one', 'CARDINAL'), ('October 21, 2011', 'DATE'), ('the Los Angeles Times', 'ORG'), ('Universal Studios', 'ORG'), ('two', 'CARDINAL'), ('Seven', 'CARDINAL'), ('Chris Morgan', 'PERSON'), ('Justin Lin', 'PERSON'), ('2006', 'DATE'), ('December 20, 2011', 'DATE'), ('Fast Five', 'ORG'), ('Vin Diesel', 'PERSON'), ('two', 'CARDINAL'), ('two', 'CARDINAL'), ('Diesel', 'ORG'), ('110', 'CARDINAL'), ('one', 'CARDINAL'), ('February 15, 2012', 'DATE'), ('Dwayne Johnson', 'PERSON'), ('two', 'CARDINAL'), ('Fast Six', 'LAW'), ('April 2013', 'DATE'), ('Fast & Furious', 'ORG'), ('6', 'CARDINAL'), ('Lin', 'PERSON'), ('seventh', 'ORDINAL'), ('summer 2014', 'DATE'), ('Lin', 'PERSON'), ('Fast & Furious', 'ORG'), ('6', 'CARDINAL'), ('two-year', 'DATE'), ('Universal', 'ORG'), ('Lin', 'PERSON'), ('sixth', 'ORDINAL'), ('April 2013', 'DATE'), ('Australian', 'NORP'), ('James Wan', 'PERSON'), ('Neal H. Moritz', 'PERSON'), ('Michael Fottrell', 'PERSON'), ('Morgan', 'PERSON'), ('fifth', 'ORDINAL'), ('April 16, 2013,', 'DATE'), ('Diesel', 'ORG'), ('July 11, 2014', 'DATE'), ('May 2013', 'DATE'), ('Diesel', 'ORG'), ('Los Angeles', 'GPE'), ('Tokyo', 'GPE'), ('the Middle East', 'LOC'), ('early September 2013', 'DATE'), ('Atlanta', 'GPE'), ('Abu Dhabi', 'ORG'), ('Dubai', 'GPE'), ('Emirate', 'ORG'), ('30%', 'PERCENT'), ('Peak Highway', 'PERSON'), ('Colorado', 'GPE'), ('September', 'DATE'), ('September 16', 'DATE'), ('Paul Walker', 'PERSON'), ('Kimsey', 'PERSON'), ('Jack', 'PERSON'), ('Atlanta', 'GPE'), ('Han', 'NORP'), ('Oakland Cemetery', 'ORG'), ('between the ages of 18 and 45', 'DATE'), ('the evening of', 'TIME'), ('September 19', 'DATE'), ('Lucas Black', 'ORG'), ('Diesel', 'ORG'), ('Atlanta', 'GPE'), ('Walker', 'ORG'), ('the same night', 'TIME'), ('one half', 'CARDINAL'), (\"Jordana Brewster's\", 'PERSON'), ('October 24', 'DATE'), ('Johnson', 'PERSON'), ('Hercules', 'GPE'), ('Five days later', 'DATE'), ('Diesel', 'ORG'), ('first', 'ORDINAL'), ('Johnson', 'PERSON'), ('November 30', 'DATE'), ('Walker', 'ORG'), (\"Brian O'Conner\", 'PERSON'), ('The next day', 'DATE'), ('Universal', 'ORG'), ('December 4', 'DATE'), ('Universal', 'ORG'), ('December 22', 'DATE'), ('Diesel', 'ORG'), ('April 10, 2015', 'DATE'), ('February 27, 2014', 'DATE'), ('Hollywood', 'GPE'), ('April 1', 'DATE'), ('Atlanta', 'GPE'), ('about eight', 'CARDINAL'), ('July 10', 'DATE'), ('Spiro Razatos', 'PERSON'), ('two', 'CARDINAL'), ('Fast Five', 'ORG'), ('Fast & Furious', 'ORG'), ('6', 'CARDINAL'), ('Business Insider', 'ORG'), ('months', 'DATE'), ('six', 'CARDINAL'), ('Lockheed', 'ORG'), ('Hercules', 'PERSON'), ('Arizona', 'GPE'), ('Colorado', 'GPE'), ('two', 'CARDINAL'), ('12,000 feet', 'QUANTITY'), ('two', 'CARDINAL'), ('BRS', 'ORG'), ('GPS', 'ORG'), ('C-130', 'FAC'), ('about 5,000 feet', 'QUANTITY'), ('10', 'CARDINAL'), ('three', 'CARDINAL'), ('Razatos', 'PERSON'), ('Three', 'CARDINAL'), ('70%', 'PERCENT'), ('30%', 'PERCENT'), ('360-degree', 'QUANTITY'), ('ten feet', 'QUANTITY'), ('about 35 to', 'CARDINAL'), ('40 miles', 'QUANTITY'), ('top\"', 'ORG'), ('Brian', 'PERSON'), ('Ramsey', 'PERSON'), ('Colorado', 'GPE'), ('Georgia', 'GPE'), ('Colorado', 'GPE'), ('340', 'CARDINAL'), ('more than 230', 'CARDINAL'), ('Mercedes-Benzes', 'ORG'), ('Ford', 'ORG'), ('Crown Victoria', 'PRODUCT'), ('Mitsubishi Montero', 'ORG'), ('W Motors', 'ORG'), ('$3.4 million', 'MONEY'), ('seven', 'CARDINAL'), ('HyperSports', 'PRODUCT'), ('Colorado', 'GPE'), ('40', 'CARDINAL'), ('Only 10 percent', 'PERCENT'), ('CGI', 'ORG'), ('more than 3,500', 'CARDINAL'), ('man-days', 'DATE'), ('Walker', 'ORG'), ('January 2014', 'DATE'), ('Time', 'ORG'), ('Walker', 'ORG'), (\"Brian O'Conner\", 'PERSON'), ('Walker', 'ORG'), (\"Peter Jackson's\", 'PERSON'), ('Weta Digital', 'ORG'), ('Rings', 'ORG'), ('Weta', 'PERSON'), ('Walker', 'ORG'), ('April 2014', 'DATE'), ('Walker', 'ORG'), ('Caleb', 'PERSON'), ('Cody', 'PERSON'), ('Walker', 'ORG'), ('Walker', 'ORG'), ('John Brotherton', 'PERSON'), ('350', 'CARDINAL'), ('260', 'CARDINAL'), ('90', 'CARDINAL'), ('Walker', 'ORG'), ('Brian Tyler', 'PERSON'), ('third', 'ORDINAL'), ('fourth', 'ORDINAL'), ('fifth', 'ORDINAL'), ('Fast & Furious 7', 'ORG'), ('Tyler', 'PERSON'), ('Atlantic Records', 'ORG'), ('March 17', 'DATE'), ('Wiz Khalifa', 'PERSON'), ('Iggy Azalea', 'PERSON'), ('Ride Out', 'WORK_OF_ART'), ('Wale, YG & Rich', 'ORG'), ('Homie Quan', 'PERSON'), ('See You Again', 'WORK_OF_ART'), ('Wiz Khalifa', 'PERSON'), ('Charlie Puth', 'PERSON'), ('Prince Royce', 'PERSON'), ('Hamdulillah', 'ORG'), ('Shadia Mansour', 'PERSON'), ('Nicky Jam', 'PERSON'), ('French', 'NORP'), ('Montana', 'GPE'), ('Tempest', 'ORG'), ('Meneo', 'ORG'), ('Fito Blanko', 'PERSON'), ('Payback', 'WORK_OF_ART'), ('Juicy J', 'PERSON'), ('Kevin Gates', 'PERSON'), ('Khalifa', 'PERSON'), (\"Charlie Puth's\", 'PERSON'), ('See You Again', 'WORK_OF_ART'), ('Paul Walker', 'PERSON'), ('the BBC Music Awards', 'ORG'), ('73rd', 'ORDINAL'), ('Golden Globe Awards', 'EVENT'), ('See You Again', 'WORK_OF_ART'), ('2015', 'DATE'), ('20.9 million', 'CARDINAL'), ('July 11, 2014', 'DATE'), (\"Paul Walker's\", 'PERSON'), ('November 2013', 'DATE'), ('October 2014', 'DATE'), ('Universal', 'ORG'), ('Furious 7', 'WORK_OF_ART'), ('seven-second', 'TIME'), ('7 Seconds of 7\"', 'WORK_OF_ART'), ('April 10, 2015', 'DATE'), ('a week to April 3', 'DATE'), ('July 2014', 'DATE'), ('7', 'CARDINAL'), ('the SXSW Film Festival', 'EVENT'), ('12:07 a.m.', 'TIME'), ('Austin', 'GPE'), ('Paramount Theatre', 'ORG'), ('March 16, 2015', 'DATE'), ('the TCL Chinese Theatre', 'ORG'), ('Los Angeles', 'GPE'), ('April 1', 'DATE'), ('IMAX Corporation', 'ORG'), ('first', 'ORDINAL'), ('U.S.', 'GPE'), ('second', 'ORDINAL'), ('2014', 'DATE'), ('Scotiabank Theatre', 'ORG'), ('Toronto', 'GPE'), ('December 2014', 'DATE'), ('7', 'CARDINAL'), ('July 6, 2015', 'DATE'), ('UK', 'GPE'), ('July 28', 'DATE'), ('140 minutes', 'TIME'), ('Wiz Khalifa', 'PERSON'), (\"Charlie Puth's\", 'PERSON'), ('See You Again', 'WORK_OF_ART'), ('Iggy Azalea', 'PERSON'), ('U.S.', 'GPE'), ('Canada', 'GPE'), ('roughly 2.5 million', 'CARDINAL'), ('its first week', 'DATE'), ('2015', 'DATE'), ('Jurassic World', 'ORG'), ('2015', 'DATE'), ('the following month', 'DATE'), ('7', 'CARDINAL'), ('$353 million', 'MONEY'), ('the United States', 'GPE'), ('Canada', 'GPE'), ('$1.163 billion', 'MONEY'), ('$1.516 billion', 'MONEY'), ('$190–250 million', 'MONEY'), ('$1 billion', 'MONEY'), ('17 days', 'DATE'), ('first', 'ORDINAL'), ('$1 million', 'MONEY'), ('Deadline Hollywood', 'PERSON'), ('$354 million', 'MONEY'), ('fifth', 'ORDINAL'), ('2015', 'DATE'), ('Most Valuable', 'WORK_OF_ART'), ('Worldwide, Furious 7', 'ORG'), ('810', 'CARDINAL'), ('$397.6 million', 'MONEY'), ('fifth', 'ORDINAL'), ('$20.8 million', 'MONEY'), ('North America', 'LOC'), ('the opening weekend', 'DATE'), ('the United States', 'GPE'), ('Canada', 'GPE'), ('$115 million to $150 million', 'MONEY'), ('Friday, April 3, 2015', 'DATE'), ('4,004', 'CARDINAL'), ('Universal', 'ORG'), ('first', 'ORDINAL'), ('Jurassic World', 'ORG'), ('$67.4 million', 'MONEY'), ('tenth', 'ORDINAL'), ('opening day', 'DATE'), ('Friday', 'DATE'), ('$15.8 million', 'MONEY'), ('late-night', 'TIME'), ('7 p.m.', 'TIME'), ('3,069', 'CARDINAL'), ('Universal', 'ORG'), ('late-night', 'TIME'), ('$2.2 million', 'MONEY'), ('third', 'ORDINAL'), ('Friday', 'DATE'), ('Thursday', 'DATE'), ('$51.5 million', 'MONEY'), ('fifth', 'ORDINAL'), ('Sunday, April 5', 'DATE'), ('$147.1 million', 'MONEY'), ('April', 'DATE'), ('the Easter Weekend', 'ORG'), ('Avengers', 'NORP'), ('Batman', 'ORG'), ('Justice', 'ORG'), ('$13.3 million', 'MONEY'), ('second', 'ORDINAL'), ('2D', 'ORG'), ('8%', 'PERCENT'), ('$11.5 million', 'MONEY'), ('400', 'CARDINAL'), ('PLF', 'ORG'), ('PLF', 'ORG'), ('its second weekend', 'DATE'), ('4,022', 'CARDINAL'), ('Universal Pictures', 'ORG'), ('an estimated $59.6 million', 'MONEY'), ('60%', 'PERCENT'), ('third', 'ORDINAL'), ('pre-summer', 'DATE'), ('the Fast & Furious', 'ORG'), ('ten days', 'DATE'), ('Fast & Furious', 'ORG'), ('fifteen weeks', 'DATE'), ('$238.67 million', 'MONEY'), ('second-weekend', 'DATE'), ('April', 'DATE'), ('North America', 'LOC'), ('7', 'CARDINAL'), ('April 1, 2015', 'DATE'), ('12', 'CARDINAL'), ('$16.9 million', 'MONEY'), ('22', 'CARDINAL'), ('33', 'CARDINAL'), ('April 2', 'DATE'), ('45', 'CARDINAL'), ('$43 million', 'MONEY'), ('8,407', 'CARDINAL'), ('Universal Pictures', 'ORG'), ('Thursday', 'DATE'), ('two-day', 'DATE'), ('$60 million', 'MONEY'), ('20', 'CARDINAL'), ('April 3', 'DATE'), ('$59.2 million', 'MONEY'), ('9,935', 'CARDINAL'), ('63', 'CARDINAL'), ('three-day', 'DATE'), ('$120.6 million', 'MONEY'), ('April 5', 'DATE'), ('4-day opening weekend', 'DATE'), ('$250.4 million', 'MONEY'), ('10,683', 'CARDINAL'), ('64', 'CARDINAL'), ('fourth', 'ORDINAL'), ('first', 'ORDINAL'), ('$7.5 million', 'MONEY'), ('175', 'CARDINAL'), ('April', 'DATE'), ('America', 'PERSON'), ('The Winter Soldier', 'EVENT'), ('2014', 'DATE'), ('$6.43 million', 'MONEY'), ('opening weekend', 'DATE'), ('29', 'CARDINAL'), ('Argentina', 'GPE'), ('Brazil', 'GPE'), ('Chile', 'GPE'), ('Colombia', 'GPE'), ('Egypt', 'GPE'), ('Malaysia', 'GPE'), ('Mexico', 'GPE'), ('Middle East', 'LOC'), ('Romania', 'GPE'), ('Taiwan', 'GPE'), ('Thailand', 'GPE'), ('Venezuela', 'GPE'), ('Vietnam', 'GPE'), ('its second weekend', 'DATE'), ('20.4%', 'PERCENT'), ('$198.7 million', 'MONEY'), ('China', 'GPE'), ('18,374', 'CARDINAL'), ('66', 'CARDINAL'), ('number one', 'CARDINAL'), ('63', 'CARDINAL'), ('the previous week', 'DATE'), ('three', 'CARDINAL'), ('its second weekend', 'DATE'), ('China', 'GPE'), ('Russia', 'GPE'), ('Poland', 'GPE'), ('$167.9 million', 'MONEY'), ('its third weekend', 'DATE'), ('North America', 'LOC'), ('three consecutive weekends', 'DATE'), ('China', 'GPE'), ('April 12', 'DATE'), ('midnight', 'TIME'), ('$8.05 million', 'MONEY'), ('$68.8 million', 'MONEY'), ('$5 million', 'MONEY'), ('2014', 'DATE'), ('$3.4 million', 'MONEY'), ('opening week', 'DATE'), ('April 12–19', 'DATE'), ('$245.9 million', 'MONEY'), ('the weekend', 'DATE'), ('$88.7 million', 'MONEY'), ('5,454', 'CARDINAL'), ('Friday to Sunday', 'DATE'), ('$182.4 million', 'MONEY'), ('Monday to Sunday', 'DATE'), ('Chinese', 'NORP'), ('five days', 'DATE'), ('China', 'GPE'), ('15 days', 'DATE'), ('China', 'GPE'), ('Canada', 'GPE'), ('the United States', 'GPE'), ('first', 'ORDINAL'), ('China', 'GPE'), ('more than 2 billion', 'MONEY'), ('China Film Group Corporation', 'ORG'), ('10%', 'PERCENT'), ('Latin America', 'LOC'), ('second', 'ORDINAL'), ('$200 million', 'MONEY'), ('first', 'ORDINAL'), ('Universal', 'ORG'), ('second', 'ORDINAL'), ('more than $200 million', 'MONEY'), ('The Avengers (2012', 'WORK_OF_ART'), ('U.S.', 'GPE'), ('Canada', 'GPE'), ('China', 'GPE'), ('$391.2 million', 'MONEY'), ('the United Kingdom', 'GPE'), ('$60 million', 'MONEY'), ('Mexico', 'GPE'), ('$51.7 million', 'MONEY'), ('Brazil', 'GPE'), ('$46.6 million', 'MONEY'), ('Germany', 'GPE'), ('$42.8 million', 'MONEY'), ('$39 million', 'MONEY'), ('China', 'GPE'), ('7', 'CARDINAL'), ('Walker', 'ORG'), ('Rotten Tomatoes', 'PERSON'), ('82%', 'PERCENT'), ('6.70/10', 'CARDINAL'), ('277', 'CARDINAL'), ('7', 'CARDINAL'), ('Metacritic', 'ORG'), ('67', 'CARDINAL'), ('100', 'CARDINAL'), ('50', 'CARDINAL'), ('CinemaScore', 'ORG'), ('A+', 'NORP'), ('2015', 'DATE'), ('March 16, 2015', 'DATE'), ('Ramin Setoodeh', 'PERSON'), ('four hours', 'TIME'), ('Walker', 'ORG'), ('A.O. Scott', 'PERSON'), ('The New York Times', 'ORG'), ('two and a half', 'DATE'), ('five', 'CARDINAL'), ('7', 'CARDINAL'), ('Furious', 'ORG'), ('John DeFore', 'PERSON'), ('Hollywood', 'GPE'), ('The Fate of the Furious', 'WORK_OF_ART'), ('F. Gary Gray', 'PERSON'), ('Chris Morgan', 'PERSON'), ('April 14, 2017', 'DATE'), (\"Universal Pictures 'Furious 7'\", 'ORG'), ('Vin Diesel', 'PERSON'), ('Paul Walker', 'PERSON'), ('Dwayne Johnson', 'PERSON'), ('April 3, 2015', 'DATE'), ('June 2, 2015', 'DATE'), ('The Videography Blog', 'WORK_OF_ART'), ('7', 'CARDINAL'), ('7', 'CARDINAL'), ('AllMovie\\nFurious 7', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "doc = ner(topic_text)\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85960725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE', 'FAC', 'GPE', 'TIME', 'QUANTITY', 'EVENT', 'WORK_OF_ART', 'PERSON', 'ORG', 'LANGUAGE', 'LOC', 'ORDINAL', 'NORP', 'CARDINAL'}\n"
     ]
    }
   ],
   "source": [
    "entities = set()\n",
    "for item in doc.ents:\n",
    "    entities.add(item.label_)\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "0be8cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc)\n",
    "test_list = []\n",
    "for item in doc.ents:\n",
    "    if item.label_ == \"GPE\":\n",
    "        test_list.append(item.text)\n",
    "        #print(item.text)\n",
    "\n",
    "inspect_counter = Counter(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "377f8baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('China', 8),\n",
       " ('the United States', 7),\n",
       " ('Los Angeles', 7),\n",
       " ('Tokyo', 7),\n",
       " ('Colorado', 6),\n",
       " ('Atlanta', 5),\n",
       " ('Canada', 5),\n",
       " ('Abu Dhabi', 3),\n",
       " ('Hobbs', 3),\n",
       " ('Hollywood', 3),\n",
       " ('U.S.', 3),\n",
       " ('Hercules', 2),\n",
       " ('Brazil', 2),\n",
       " ('Mexico', 2),\n",
       " ('Deckard', 1),\n",
       " ('El Segundo', 1),\n",
       " ('California', 1),\n",
       " ('Azerbaijan', 1),\n",
       " ('Miami', 1),\n",
       " ('Rousey', 1),\n",
       " ('Spain', 1),\n",
       " ('Lucas Black', 1),\n",
       " ('the Dominican Republic', 1),\n",
       " ('Dubai', 1),\n",
       " ('Arizona', 1),\n",
       " ('Georgia', 1),\n",
       " ('Montana', 1),\n",
       " ('Austin', 1),\n",
       " ('Toronto', 1),\n",
       " ('UK', 1),\n",
       " ('Argentina', 1),\n",
       " ('Chile', 1),\n",
       " ('Colombia', 1),\n",
       " ('Egypt', 1),\n",
       " ('Malaysia', 1),\n",
       " ('Romania', 1),\n",
       " ('Taiwan', 1),\n",
       " ('Thailand', 1),\n",
       " ('Venezuela', 1),\n",
       " ('Vietnam', 1),\n",
       " ('Russia', 1),\n",
       " ('Poland', 1),\n",
       " ('the United Kingdom', 1),\n",
       " ('Germany', 1)]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a737aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_w = ner_w(topic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fe9d66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MISC', 'LOC', 'PER', 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "entities_w = set()\n",
    "for item in doc_w.ents:\n",
    "    entities_w.add(item.label_)\n",
    "\n",
    "print(entities_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d76e6f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City\n",
      "Nevada\n",
      "Seattle\n",
      "Hong Kong\n",
      "Seattle\n",
      "Savannah\n",
      "Las Vegas High School\n",
      "Paris\n",
      "London\n",
      "Europe\n",
      "United States\n",
      "Vietnam\n",
      "New York\n",
      "London\n",
      "Texas\n",
      "Alvez\n",
      "Alvez\n",
      "Iraq\n",
      "Eli\n",
      "Belgium\n",
      "the Pentagon\n",
      "New York\n",
      "Las Vegas\n",
      "Grand Canyon\n",
      "Virginia\n",
      "Briscoe County\n",
      "Texas\n",
      "Texas\n",
      "Barbados\n",
      "Aruba\n",
      "Florida\n",
      "Rochelle Aytes\n",
      "Bethesda General Hospital\n",
      "Savannah\n",
      "Savannah\n",
      "Alabama\n",
      "Tennessee\n",
      "Santa Barbara\n",
      "Southeast Asia\n",
      "Hong Kong\n",
      "Taiwan\n"
     ]
    }
   ],
   "source": [
    "for item in doc_w.ents:\n",
    "    if item.label_ == \"LOC\":\n",
    "        print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d62c4",
   "metadata": {},
   "source": [
    "### 3. Text Generation via Keywords\n",
    "https://medium.com/mlearning-ai/generating-sentences-from-keywords-using-transformers-in-nlp-e89f4de5cf6b\n",
    "\n",
    "Generation Idea: \n",
    "* Use Monopoly action cards in combination with terms of wikipedia article to generate new action cards DONE\n",
    "* Group monopoly cards into sentiments: positive, negative, neutral DONE\n",
    "* Create frequency count of words in original action cards, use POS tagging / NER\n",
    "* remove entities in monopoly action cards and save which entities have been removed (places?, persons?) -> do it deliberatly?\n",
    "* filter stop words?\n",
    "\n",
    "now either:\n",
    "- Entity Swap (Kazemi): select removed entities from wikipedia article -> use frequency counts to select \"most important ones\" (higher frequency -> more important entity), swap entities\n",
    "- Text Generation from Keywords: generate new text via k2t model and selected input words\n",
    "\n",
    "Evaluation idea:\n",
    "* analyse distribution of POS tags in in original monopoly cards vs. distribution of POS tags in generated action cards? Compare two distributions via Kullback-Leibler-Divergenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8924c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keytotext import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8e8866ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "184815b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "aae01b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_MONOPOLY = \"monopoly_action_cards_keywords.csv\"\n",
    "monopoly_data = pd.read_csv(FILENAME_MONOPOLY, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5b135f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>bias</th>\n",
       "      <th>keywords</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Event card</td>\n",
       "      <td>Pay a fine of DM 200,- or take a community tic...</td>\n",
       "      <td>negativ</td>\n",
       "      <td>pay, fine, community ticket</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Event card</td>\n",
       "      <td>Move up to Seestrasse. \\nIf you come over Go, ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>move, Seestrasse, go</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Event card</td>\n",
       "      <td>Go back 3 fields.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>go, fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Event card</td>\n",
       "      <td>Go back to Badstraße.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>go, Badstraße</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Event card</td>\n",
       "      <td>Move forward to Schlossallee.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>move, Schlossallee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                            content     bias  \\\n",
       "0  Event card  Pay a fine of DM 200,- or take a community tic...  negativ   \n",
       "1  Event card  Move up to Seestrasse. \\nIf you come over Go, ...  neutral   \n",
       "2  Event card                                  Go back 3 fields.  neutral   \n",
       "3  Event card                              Go back to Badstraße.  neutral   \n",
       "4  Event card                      Move forward to Schlossallee.  neutral   \n",
       "\n",
       "                      keywords  Unnamed: 4  Unnamed: 5  Unnamed: 6  \n",
       "0  pay, fine, community ticket         NaN         NaN         NaN  \n",
       "1         move, Seestrasse, go         NaN         NaN         NaN  \n",
       "2                   go, fields         NaN         NaN         NaN  \n",
       "3                go, Badstraße         NaN         NaN         NaN  \n",
       "4           move, Schlossallee         NaN         NaN         NaN  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monopoly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fa52a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter monopoly data by sentiment\n",
    "monopoly_data_pos = monopoly_data[monopoly_data[\"bias\"] == \"positiv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b0eef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_counts = dict()\\nfor size in 2, 3, 4, 5:\\n    all_counts[size] = FreqDist(ngrams(monopoly_data_pos[\"content\"], size))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk import ngrams, FreqDist\n",
    "\n",
    "#inspect_counts = FreqDist(ngrams(monopoly_data_pos[\"content\"], 1))\n",
    "\n",
    "\"\"\"\n",
    "all_counts = dict()\n",
    "for size in 2, 3, 4, 5:\n",
    "    all_counts[size] = FreqDist(ngrams(monopoly_data_pos[\"content\"], size))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c22eccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      "{'the': 56, 'bank': 13, 'pays': 41, 'you': 62, 'dividend': 22, 'dm': 23, '1000': 0, 'rent': 50, 'and': 9, 'bond': 17, 'interest': 33, 'are': 12, 'due': 25, '3000': 4, 'receive': 47, 'on': 39, 'preferred': 43, 'shares': 53, '900': 8, 'inherit': 32, '2000': 2, 'from': 29, 'stock': 54, 'sales': 51, '500': 7, 'annual': 10, 'annuity': 11, 'is': 34, 'draw': 24, 'win': 60, 'crossword': 21, 'puzzle': 46, 'contest': 20, 'error': 27, 'in': 30, 'your': 63, 'favor': 28, '4000': 6, 'income': 31, 'tax': 55, 'refund': 48, '400': 5, 'won': 61, '2nd': 3, 'prize': 45, 'beauty': 15, '200': 1, 'it': 35, 'birthday': 16, 'collect': 19, 'each': 26, 'player': 42, 'will': 59, 'be': 14, 'released': 49, 'prison': 44, 'must': 37, 'keep': 36, 'this': 57, 'card': 18, 'until': 58, 'need': 38, 'or': 40, 'sell': 52}\n"
     ]
    }
   ],
   "source": [
    "# Create our vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(monopoly_data_pos[\"content\"])\n",
    "\n",
    "# Let's look at the vocabulary:\n",
    "print('Vocabulary: ')\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "800ff93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save frequencies of words in monopoly action cards\n",
    "new_list = []\n",
    "\n",
    "for key, value in vectorizer.vocabulary_.items():\n",
    "    new_series = [key, value]\n",
    "    new_list.append(new_series)\n",
    "\n",
    "action_cards = pd.DataFrame(new_list, columns=[\"word\",\"frequency\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55b506ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>your</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>won</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>win</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>will</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2nd</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  frequency\n",
       "35  your         63\n",
       "3    you         62\n",
       "42   won         61\n",
       "29   win         60\n",
       "52  will         59\n",
       "..   ...        ...\n",
       "13  3000          4\n",
       "43   2nd          3\n",
       "20  2000          2\n",
       "46   200          1\n",
       "6   1000          0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_cards.sort_values(by=\"frequency\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8517f5",
   "metadata": {},
   "source": [
    "## POS-Tag Preprocessing for Monopoly Action Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04605ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sent = preprocess(monopoly_data_pos[\"content\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "b56bf656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VB', 'DT', 'NN', 'IN', 'NN', 'CD', ',', ':', 'CC', 'VB', 'DT', 'NN', 'NN', '.']\n",
      "Counter({'NN': 4, 'VB': 2, 'DT': 2, 'IN': 1, 'CD': 1, ',': 1, ':': 1, 'CC': 1, '.': 1})\n",
      "['NN', 'RB', 'TO', 'NN', '.', 'IN', 'PR', 'VB', 'IN', 'NN', ',', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 5, '.': 2, 'IN': 2, ',': 2, 'RB': 1, 'TO': 1, 'PR': 1, 'VB': 1, 'CD': 1, ':': 1})\n",
      "['NN', 'RB', 'CD', 'NN', '.']\n",
      "Counter({'NN': 2, 'RB': 1, 'CD': 1, '.': 1})\n",
      "['VB', 'RB', 'TO', 'NN', '.']\n",
      "Counter({'VB': 1, 'RB': 1, 'TO': 1, 'NN': 1, '.': 1})\n",
      "['NN', 'RB', 'TO', 'NN', '.']\n",
      "Counter({'NN': 2, 'RB': 1, 'TO': 1, '.': 1})\n",
      "['NN', 'RB', 'TO', 'NN', '.']\n",
      "Counter({'NN': 2, 'RB': 1, 'TO': 1, '.': 1})\n",
      "['VB', 'DT', 'NN', 'TO', 'DT', 'JJ', 'NN', '.', 'WR', 'PR', 'VB', 'IN', 'NN', ',', 'RB', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 4, 'VB': 2, 'DT': 2, '.': 2, ',': 2, 'TO': 1, 'JJ': 1, 'WR': 1, 'PR': 1, 'IN': 1, 'RB': 1, 'CD': 1, ':': 1})\n",
      "['DT', 'NN', 'VB', 'PR', 'DT', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 3, 'DT': 2, 'VB': 1, 'PR': 1, 'CD': 1, ',': 1, ':': 1, '.': 1})\n",
      "['NN', 'CC', 'NN', 'NN', 'VB', 'JJ', '.', 'DT', 'NN', 'VB', 'PR', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 5, 'VB': 2, '.': 2, 'CC': 1, 'JJ': 1, 'DT': 1, 'PR': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['NN', 'RB', 'TO', 'NN', '.']\n",
      "Counter({'NN': 2, 'RB': 1, 'TO': 1, '.': 1})\n",
      "['PR', 'VB', 'DT', 'CD', 'NN', 'NN', 'IN', 'JJ', 'NN', '.', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 4, 'CD': 2, '.': 2, 'PR': 1, 'VB': 1, 'DT': 1, 'IN': 1, 'JJ': 1, ',': 1, ':': 1})\n",
      "['PR', 'VB', ':', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({':': 2, 'PR': 1, 'VB': 1, 'NN': 1, 'CD': 1, ',': 1, '.': 1})\n",
      "['IN', 'NN', 'NN', 'PR', 'VB', ':', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 3, ':': 2, 'IN': 1, 'PR': 1, 'VB': 1, 'CD': 1, ',': 1, '.': 1})\n",
      "['DT', 'JJ', 'NN', 'VB', 'JJ', '.', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 3, 'JJ': 2, '.': 2, 'DT': 1, 'VB': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['PR', 'VB', 'DT', 'NN', 'NN', 'NN', '.', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 5, '.': 2, 'PR': 1, 'VB': 1, 'DT': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['NN', 'NN', 'IN', 'PR', 'NN', '.', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 5, '.': 2, 'IN': 1, 'PR': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['JJ', 'NN', 'NN', '.', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 4, '.': 2, 'JJ': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['PR', 'VB', 'DT', 'CD', 'NN', 'IN', 'DT', 'NN', 'NN', '.', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 5, 'DT': 2, 'CD': 2, '.': 2, 'PR': 1, 'VB': 1, 'IN': 1, ',': 1, ':': 1})\n",
      "['PR', 'VB', 'PR', 'NN', '.', 'NN', 'NN', 'CD', ',', ':', 'IN', 'DT', 'NN', '.']\n",
      "Counter({'NN': 4, 'PR': 2, '.': 2, 'VB': 1, 'CD': 1, ',': 1, ':': 1, 'IN': 1, 'DT': 1})\n",
      "['NN', 'TO', 'DT', 'JJ', 'NN', '.', 'DT', 'NN', 'VB', 'RB', 'DT', 'JJ', 'NN', '.', 'IN', 'DT', 'NN', 'VB', 'DT', 'NN', 'TO', 'DT', 'NN', 'RB', ',', 'PR', 'MD', 'VB', 'PR', 'IN', 'DT', 'NN', '.']\n",
      "Counter({'NN': 8, 'DT': 7, '.': 3, 'VB': 3, 'TO': 2, 'JJ': 2, 'RB': 2, 'IN': 2, 'PR': 2, ',': 1, 'MD': 1})\n",
      "['NN', 'NN', 'NN', ':', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 4, ':': 2, 'CD': 1, ',': 1, '.': 1})\n",
      "['NN', 'PO', 'NN', '.', 'NN', ':', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 4, '.': 2, ':': 2, 'PO': 1, 'CD': 1, ',': 1})\n",
      "['NN', 'TO', 'DT', 'NN', 'NN', '.', 'IN', 'PR', 'VB', 'IN', 'NN', ',', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 6, '.': 2, 'IN': 2, ',': 2, 'TO': 1, 'DT': 1, 'PR': 1, 'VB': 1, 'CD': 1, ':': 1})\n",
      "['NN', 'IN', 'NN', ':', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 3, ':': 2, 'IN': 1, 'CD': 1, ',': 1, '.': 1})\n",
      "['PR', 'VB', 'VB', 'VB', 'TO', 'DT', 'NN', '.', 'NN', 'DT', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 4, 'VB': 3, 'DT': 2, '.': 2, 'PR': 1, 'TO': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['VB', 'DT', 'PR', 'NN', 'VB', '.', 'NN', 'TO', 'DT', 'NN', '.', 'IN', 'DT', 'NN', 'VB', 'CD', ',', ':', 'IN', 'DT', 'NN', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 6, 'DT': 4, 'VB': 3, '.': 3, 'IN': 2, 'CD': 2, ',': 2, ':': 2, 'PR': 1, 'TO': 1})\n",
      "['NN', 'TO', 'DT', 'NN', ':', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 3, ':': 2, 'TO': 1, 'DT': 1, 'CD': 1, ',': 1, '.': 1})\n",
      "['PR', 'MD', 'VB', 'VB', 'IN', 'TO', 'VB', 'NN', 'NN', 'NN', '.', 'VB', 'IN', 'PR', 'NN', 'CC', 'NN', '.', 'NN', 'CD', ',', ':', 'IN', 'NN', '.', 'NN', 'CD', ',', ':', 'IN', 'NN', 'TO', 'DT', 'NN', '.']\n",
      "Counter({'NN': 10, 'VB': 4, 'IN': 4, '.': 4, 'PR': 2, 'TO': 2, 'CD': 2, ',': 2, ':': 2, 'MD': 1, 'CC': 1, 'DT': 1})\n",
      "['VB', 'TO', 'DT', 'NN', '.', 'NN', 'RB', 'RB', '.', 'VB', 'RB', 'VB', 'NN', '.', 'NN', 'RB', 'VB', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 5, 'VB': 4, '.': 4, 'RB': 4, 'TO': 1, 'DT': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['PR', 'MD', 'VB', 'VB', 'IN', 'NN', '.', 'PR', 'MD', 'VB', 'DT', 'NN', 'IN', 'PR', 'VB', 'PR', 'CC', 'VB', 'PR', '.']\n",
      "Counter({'PR': 5, 'VB': 5, 'MD': 2, 'IN': 2, 'NN': 2, '.': 2, 'DT': 1, 'CC': 1})\n",
      "['VB', 'TO', 'DT', 'NN', '.', 'NN', 'RB', 'RB', '.', 'VB', 'RB', 'VB', 'NN', '.', 'NN', 'RB', 'VB', 'NN', 'CD', ',', ':', '.']\n",
      "Counter({'NN': 5, 'VB': 4, '.': 4, 'RB': 4, 'TO': 1, 'DT': 1, 'CD': 1, ',': 1, ':': 1})\n",
      "['PR', 'MD', 'VB', 'VB', 'IN', 'NN', '.', 'PR', 'MD', 'VB', 'DT', 'NN', 'IN', 'PR', 'VB', 'PR', 'CC', 'VB', 'PR', '.']\n",
      "Counter({'PR': 5, 'VB': 5, 'MD': 2, 'IN': 2, 'NN': 2, '.': 2, 'DT': 1, 'CC': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "imd_list = []\n",
    "collect_counts = []\n",
    "for action_text in monopoly_data[\"content\"]:\n",
    "    action_pos_tags = preprocess(action_text)\n",
    "    #print(action_pos_tags)\n",
    "    imd_list = []\n",
    "    for _, pos_tag in action_pos_tags:\n",
    "        #print(pos_tag)\n",
    "        imd_list.append(pos_tag[0:2])\n",
    "    print(imd_list)\n",
    "    print(Counter(imd_list))\n",
    "    #collect_counts.append()\n",
    "    \n",
    "    #\n",
    "#print(collect_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5f8192ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sent = preprocess(action_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd52e6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('bank', 'NN'),\n",
       " ('pays', 'VBZ'),\n",
       " ('you', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('dividend', 'NN'),\n",
       " ('.', '.'),\n",
       " ('DM', 'NNP'),\n",
       " ('1000', 'CD'),\n",
       " (',', ','),\n",
       " ('-', ':')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "961cf5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('bank', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('dividend', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Euro', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('1000', 'CD'),\n",
       " ('in', 'IN'),\n",
       " ('Texas', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "804adb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "NN\n",
      "VB\n",
      "PR\n",
      "DT\n",
      "NN\n",
      ".\n",
      "NN\n",
      "CD\n",
      ",\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "## pos distribution\n",
    "pos_tag_df_reference = pd.DataFrame(output_sent,columns=[\"token\",\"pos_tag\"])\n",
    "for x in pos_tag_df_reference[\"pos_tag\"]:\n",
    "    print(x[0:2])\n",
    "pos_tag_df_reference[\"short_pos_tag\"] = [x[0:2] for x in pos_tag_df_reference[\"pos_tag\"]]\n",
    "\n",
    "reference = pos_tag_df_reference[\"short_pos_tag\"].value_counts()\n",
    "\n",
    "pos_tag_df_target = pd.DataFrame(action_sent,columns=[\"token\",\"pos_tag\"])\n",
    "\n",
    "pos_tag_df_target[\"short_pos_tag\"] = [x[0:2] for x in pos_tag_df_target[\"pos_tag\"]]\n",
    "\n",
    "target = pos_tag_df_target[\"short_pos_tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6af915de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7705517503711221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.77055175]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(reference,target,how=\"outer\", left_index=True,right_index=True)\n",
    "merged_df.columns=[\"reference\",\"target\"]\n",
    "merged_df = merged_df.fillna(0)\n",
    "#np.array(merged_df[\"reference\"])\n",
    "#np.dot(merged_df[\"reference\"], merged_df[\"target\"])\n",
    "print(1 - cosine(merged_df[\"reference\"], merged_df[\"target\"]))\n",
    "#cosine_similarity([merged_df[\"reference\"]], [merged_df[\"target\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "003ac75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic's keyword_list is:  ['bank', 'pays', 'you', 'dividend', 'Euro', '1000']\n"
     ]
    }
   ],
   "source": [
    "## use only verbs, personal pronoun, adjectives, nouns\n",
    "keyword_list = []\n",
    "\n",
    "for pos_tag in output_sent:\n",
    "    if re.match(\"VB.*|NN.*|PR.*|RB.*|CD.*\", pos_tag[1]):\n",
    "        if pos_tag[0] == \"DM\":\n",
    "            keyword_list.append(\"Euro\")\n",
    "        else:\n",
    "            keyword_list.append(pos_tag[0])  \n",
    "        \n",
    "print(\"The topic's keyword_list is: \", keyword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26714b1e",
   "metadata": {},
   "source": [
    "List of POS-Tag Description:\n",
    "https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95988e",
   "metadata": {},
   "source": [
    "## Preprocess NER tokens for Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64609ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORG'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_w.ents[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34a13e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_collection = []\n",
    "for item in doc_w.ents:\n",
    "    ent_collection.append([item.text,item.label_])\n",
    "\n",
    "topic_data_entities = pd.DataFrame(ent_collection,columns=[\"entity\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "961a6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(topic_data_entities[\"entity\"].value_counts())\n",
    "count_df.columns=[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a8f15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data_entities = pd.merge(topic_data_entities,count_df,how=\"left\",left_on=\"entity\",right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d79806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## distinct rows after join\n",
    "topic_data_entities = topic_data_entities.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a553cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic = topic_data_entities[topic_data_entities[\"label\"]==\"LOC\"].sort_values(by=\"count\",ascending=False).entity.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ad81b",
   "metadata": {},
   "source": [
    "- POS-Tag on Monopoly Data + Counts\n",
    "- NER on Wikipedia Data + Count\n",
    "- NER on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "22d5861e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bank', 'pays', 'you', 'dividend', 'Euro', '1000', 'Texas']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_list.append(first_topic)\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf4f5f",
   "metadata": {},
   "source": [
    "## Key-to-Text Model Application\n",
    "\n",
    "Input: List of words from action cards + wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f19bd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_k2t_base = pipeline(\"k2t-base\")  #loading the pre-trained model\n",
    "params = {\"do_sample\":True, \"num_beams\":4, \"no_repeat_ngram_size\":3, \"early_stopping\":True}    #decoding params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0f9c83f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8f9d8a6b8340ac8ca499df0f1dac7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371bf8ef06ae4a90893a540292efccae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e868f626dcc42318c48fe8c81577c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keytotext import pipeline\n",
    "nlp_k2t = pipeline(\"k2t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c04cb617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bank with a dividend of Euro is 1000 in Texas.\n"
     ]
    }
   ],
   "source": [
    "keywords=[\"You\", \"do\", \"Move forward to\",\"Galactic Republic\"]\n",
    "action_output = (nlp(keyword_list, **params)) \n",
    "print(action_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5f41eb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic's keyword_list is:  ['Pay', 'take', 'come', 'Go', 'Go', 'Go', 'get', 'pays', 'are', 'pays', 'Go', 'receive', 'inherit', 'receive', 'is', 'win', 'won', 'is', '..', 'receives', 'has', 'buy', 'get', 'have', 'been', 'elected', 'Have', 'renovated', 'Euro', 'Pay', 'be', 'called', 'do', 'Pay', 'Do', 'pass', 'collect', 'be', 'released', 'keep', 'need', 'sell', 'Do', 'pass', 'collect', 'be', 'released', 'keep', 'need', 'sell']\n",
      "\n",
      "\n",
      "The topic's keyword_list is:  ['fine', 'Euro', 'community', 'ticket', '..', 'Move', 'Seestrasse', 'Go', 'collect', 'Euro', 'fields', 'Badstraße', 'Move', 'Schlossallee', '..', 'Move', '..', 'Make', 'trip', 'station', 'Go', 'Euro', 'bank', 'dividend', 'Euro', 'Rent', 'bond', 'interest', 'bank', 'Euro', 'Move', '..', '%', 'dividend', 'shares', 'Euro', 'Euro', 'stock', 'sales', 'Euro', 'annuity', 'Draw', 'Euro', 'crossword', 'puzzle', 'contest', 'Draw', 'Euro', 'Bank', 'error', 'favor', 'Draw', 'Euro', 'tax', 'refund', 'Draw', 'Euro', 'prize', 'beauty', 'contest', 'Draw', 'Euro', 'birthday', 'Collect', 'Euro', 'player', 'Advance', 'station', 'owner', 'rent', 'player', 'claim', 'station', 'bank', 'Pay', 'school', 'fees', 'Euro', 'Doctor', 'fees', 'Pay', 'Euro', 'Advance', 'opera', 'square', 'Go', 'collect', 'Euro', 'Fine', 'speeding', 'Euro', 'board', 'Pay', 'player', 'Euro', 'houses', 'Pay', 'bank', 'house', 'hotel', 'Euro', 'hospital', 'Euro', 'road', 'repair', 'work', 'houses', 'hotels', 'Euro', 'house', 'Euro', 'hotel', 'bank', '..', 'Go', 'prison', 'Go', 'Go', 'Do', 'Euro', '..', 'prison', 'card', 'Go', 'prison', 'Go', 'Go', 'Do', 'Euro', '..', 'prison', 'card']\n",
      "\n",
      "\n",
      "The topic's keyword_list is:  ['you', 'you', 'you', 'you', 'You', 'You', 'you', 'You', 'your', 'You', 'It', 'your', 'you', 'it', 'you', 'You', 'your', 'You', 'your', 'You', 'You', 'you', 'it', 'it', 'You', 'You', 'you', 'it', 'it']\n",
      "\n",
      "\n",
      "The topic's keyword_list is:  ['a', 'a', 'a', 'the', 'The', 'a', 'The', 'a', 'The', 'a', 'the', 'a', 'each', 'the', 'The', 'the', 'no', 'a', 'this', 'the', 'the', 'the', 'each', 'all', 'the', 'each', 'each', 'the', 'the', 'the', 'this', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "## TESTING\n",
    "\n",
    "text_data = \"\"\n",
    "for text_item in monopoly_data[\"content\"]:\n",
    "    text_data += \". \" + text_item\n",
    "    \n",
    "inspect_actions = preprocess(text_data)\n",
    "\n",
    "## use only verbs, personal pronoun, adjectives, nouns\n",
    "keyword_list_verbs = []\n",
    "\n",
    "for pos_tag in inspect_actions:\n",
    "    if re.match(\"VB.*\", pos_tag[1]):\n",
    "        if pos_tag[0] == \"DM\":\n",
    "            keyword_list_verbs.append(\"Euro\")\n",
    "        else:\n",
    "            keyword_list_verbs.append(pos_tag[0])  \n",
    "        \n",
    "print(\"The topic's keyword_list is: \", keyword_list_verbs)\n",
    "\n",
    "keyword_list_nouns = []\n",
    "\n",
    "for pos_tag in inspect_actions:\n",
    "    if re.match(\"NN.*\", pos_tag[1]):\n",
    "        if pos_tag[0] == \"DM\":\n",
    "            keyword_list_nouns.append(\"Euro\")\n",
    "        else:\n",
    "            keyword_list_nouns.append(pos_tag[0])  \n",
    "        \n",
    "print(\"\\n\\nThe topic's keyword_list is: \", keyword_list_nouns)\n",
    "\n",
    "\n",
    "keyword_list_pronouns = []\n",
    "\n",
    "for pos_tag in inspect_actions:\n",
    "    if re.match(\"PR.*\", pos_tag[1]):\n",
    "        if pos_tag[0] == \"DM\":\n",
    "            keyword_list_pronouns.append(\"Euro\")\n",
    "        else:\n",
    "            keyword_list_pronouns.append(pos_tag[0])  \n",
    "        \n",
    "print(\"\\n\\nThe topic's keyword_list is: \", keyword_list_pronouns)\n",
    "\n",
    "\n",
    "keyword_list_determiner = []\n",
    "\n",
    "for pos_tag in inspect_actions:\n",
    "    if re.match(\"DT.*\", pos_tag[1]):\n",
    "        if pos_tag[0] == \"DM\":\n",
    "            keyword_list_determiner.append(\"Euro\")\n",
    "        else:\n",
    "            keyword_list_determiner.append(pos_tag[0])  \n",
    "        \n",
    "print(\"\\n\\nThe topic's keyword_list is: \", keyword_list_determiner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7d74b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "637baf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "02f782d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_verb = \"pass\"\n",
    "pronoun = \"you\"\n",
    "LOCATION = \"Harry Potter Street\"\n",
    "ACTOR = \"Hermine Granger\"\n",
    "number = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "86d2bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arrange', 'pass', 'you', 'Harry Potter Street', 'Hermine Granger']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/generation_utils.py:2142: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The arrangement for the episode with hermine granger is \"president\".\n"
     ]
    }
   ],
   "source": [
    "keywords=[first_verb, second_verb, pronoun, LOCATION, ACTOR]\n",
    "print(keywords)\n",
    "#keywords=[\"You\", \"do\", \"Move forward to\",\"Galactic Republic\"]\n",
    "#action_output = (nlp(keywords, **params)) \n",
    "action_output = (nlp_k2t(keywords, **params)) \n",
    "\n",
    "print(action_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aad16761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The arrangement for the episode with hermine granger is \"president\". She was part of the team that played the villain in her own series with Brawn.\\n\\nAce: Originally one of'},\n",
       " {'generated_text': 'The arrangement for the episode with hermine granger is \"president\".\\n\\n\"When she gets down there in the hallway that I would like to call [Ms. Grace] the President was originally'},\n",
       " {'generated_text': 'The arrangement for the episode with hermine granger is \"president\".\\n\\nPuppet Shows\\n\\nAvengers: Infinity War took a more \"traditional\" approach to a series of interl'},\n",
       " {'generated_text': 'The arrangement for the episode with hermine granger is \"president\". There is also an audio recording of the performance by director Mark Rylance – a man she met while filming the series for the'},\n",
       " {'generated_text': 'The arrangement for the episode with hermine granger is \"president\". \"So you know what would happen?\" she asks. \"My wife would have to work for eight to 10 hours a day to'}]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from transformers import pipeline, set_seed\n",
    "#generator = pipeline('text-generation', model='gpt2')\n",
    "#set_seed(42)\n",
    "generator(\"The arrangement for the episode with hermine granger is \\\"president\\\".\", max_length=40, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410926d1",
   "metadata": {},
   "source": [
    "## Evaluation of Generated Action Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9e0084b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_distribution(pos_tuples_of_sentence):\n",
    "    \"\"\"\n",
    "    :pos_tuple_of_sentences: tuple (token, pos_tag) as returned from preprocess function\n",
    "    \n",
    "    crop pos tags into relevant groups (first two letters)\n",
    "    count occurences of pos tags in input sentence\n",
    "    \n",
    "    :returns: pandas DataFrame with pos_tag and its frequency\n",
    "    \n",
    "    \"\"\"\n",
    "    pos_df = pd.DataFrame(pos_tuples_of_sentence,columns=[\"token\",\"long_pos_tag\"])\n",
    "    pos_df[\"pos_tag\"] = [x[0:2] for x in pos_df[\"long_pos_tag\"]]\n",
    "    freq_df = pos_df[\"pos_tag\"].value_counts()\n",
    "    \n",
    "    return freq_df\n",
    "\n",
    "def evaluate_generated_sentence(reference, new_sentence):\n",
    "\n",
    "    ## preprocess both\n",
    "    reference = preprocess(reference)\n",
    "    new_sentence = preprocess(new_sentence)\n",
    "    \n",
    "    ## pos distribution\n",
    "    reference = pos_distribution(reference)\n",
    "    new_sentence = pos_distribution(new_sentence)\n",
    "    \n",
    "    ## merge vectors\n",
    "    merged_df = pd.merge(reference,new_sentence,how=\"outer\", left_index=True,right_index=True).fillna(0)\n",
    "    merged_df.columns=[\"reference\",\"target\"]\n",
    "    \n",
    "    ## calc cosine similarity \n",
    "    cos_similarity = 1 - cosine(merged_df[\"reference\"], merged_df[\"target\"])\n",
    "    \n",
    "    ## calc scalar product between vectors\n",
    "    return cos_similarity\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS of output sentence -> to make sure there is an action in it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee6c04",
   "metadata": {},
   "source": [
    "## NER on Monopoly Data\n",
    "### Check named entity swap? Similar to Kazemi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c7dc676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The bank pays you a dividend. DM 1000,- Rent and bond interest are due. The bank pays you DM 3000,- You receive a 7% dividend on preferred shares. DM 900,- You inherit: DM 2000,- From stock sales you receive: DM 500,- The annual annuity is due. Draw DM 2000,- You win a crossword puzzle contest. Draw DM 2000,- Bank error in your favor. Draw DM 4000,- Income tax refund. Draw DM 400,- You won the 2nd prize in a beauty contest. Draw DM 200,- It is your birthday. Collect DM 1000,- from each player. You will be released from prison! You must keep this card until you need it or sell it. You will be released from prison! You must keep this card until you need it or sell it.\n",
      "[('1000,-', 'CARDINAL'), ('7%', 'PERCENT'), ('DM 900,-', 'ORG'), ('2000,-', 'CARDINAL'), ('500,-', 'PRODUCT'), ('annual', 'DATE'), ('2000,-', 'CARDINAL'), ('1000,-', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "text_data = \"\"\n",
    "for text_item in monopoly_data_pos[\"content\"]:\n",
    "    text_data += \" \" + text_item\n",
    "\n",
    "print(text_data)\n",
    "    \n",
    "#sent = \n",
    "#print(sent)\n",
    "sent_ner = ner(text_data)\n",
    "print([(X.text, X.label_) for X in sent_ner.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fdb15be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bank pays you a dividend. DM 1000,-Rent and bond interest are due. The bank pays you DM 3000,-You receive a 7% dividend on preferred shares. DM 900,-You inherit: DM 2000,-From stock sales you receive: DM 500,-The annual annuity is due. Draw DM 2000,-You win a crossword puzzle contest. Draw DM 2000,-Bank error in your favor. Draw DM 4000,-Income tax refund. Draw DM 400,-You won the 2nd prize in a beauty contest. Draw DM 200,-It is your birthday. Collect DM 1000,- from each player.You will be released from prison! You must keep this card until you need it or sell it.You will be released from prison! You must keep this card until you need it or sell it.\n"
     ]
    }
   ],
   "source": [
    "print(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7e169",
   "metadata": {},
   "source": [
    "## Few-Shot-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c471a1",
   "metadata": {},
   "source": [
    "Additional Data Sources:\n",
    "https://www.kaggle.com/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_fictional_towns_in_television\n",
    "\n",
    "Source Inference API: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5133cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee517d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"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\"\n",
    "\n",
    "def query(payload='',parameters=None,options={'use_cache': False}):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    body = {\"inputs\":payload,'parameters':parameters,'options':options}\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data= json.dumps(body))\n",
    "    try:\n",
    "      response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        return \"Error:\"+\" \".join(response.json()['error'])\n",
    "    else:\n",
    "      return response.json()[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2b593467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate few shot training data for text generation\n",
    "\n",
    "prompt_text = \"\"\n",
    "\n",
    "for text, keywords in zip(monopoly_data[\"content\"], monopoly_data[\"keywords\"]):\n",
    "    imd = \"key: \" + keywords + \"\\ntweet: \" + text + \"\\n###\"\n",
    "    prompt_text += imd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "c2bbbd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first_verb: being released \n",
      "second_verb: see \n",
      "pronoun: your \n",
      "LOCATION: Dominic Toretto Avenue \n",
      "number: 2000\n",
      "\n",
      "select_second_verb: 1 \n",
      "select_pronoun: 0 \n",
      "select_location: 1\n",
      "['being released', 'see', 'Dominic Toretto Avenue']\n",
      "being released, see, Dominic Toretto Avenue\n"
     ]
    }
   ],
   "source": [
    "## Source Action Verbs: https://www.citationmachine.net/resources/grammar-guides/verb/list-verbs/\n",
    "\n",
    "action_verbs = [\"Act\",\"Answer\",\"Approve\",\"Arrange\",\"Break\",\"Build\",\"Buy\",\"Coach\",\"Color\",\"Cough\",\"Create\", \n",
    "                \"Complete\",\"Cry\",\"Dance\",\"Describe\",\"Draw\",\"Drink\",\"Eat\",\"Edit\",\"Enter\",\"Exit\",\n",
    "                \"Imitate\",\"Invent\",\"Jump\",\"Laugh\",\"Lie\",\"Listen\",\"Paint\",\"Plan\",\"Play\",\"Read\",\"Replace\",\n",
    "                \"Run\",\"Scream\",\"See\",\"Shop\",\"Shout\",\"Sing\",\"Skip\",\"Sleep\",\"Sneeze\",\"Solve\",\"Study\",\"Teach\",\n",
    "                \"Touch\",\"Turn\",\"Walk\",\"Win\",\"Write\",\"Whistle\",\"Yank\",\"Zip\"]\n",
    "\n",
    "## POS Tag == \"VB.*\" from real monopoly action cards\n",
    "action_verbs_monopoly = [\"Pay\",\"Take\",\"Come\",\"Go\",\"Get\",\"Receive\",\"Inherit\",\"Win\",\"Pass\",\n",
    "                         \"Collect\",\"being released\",\"Keep\",\"Sell\"]\n",
    "\n",
    "\n",
    "## randomly select verbs, pronouns\n",
    "first_verb = random.choice(action_verbs_monopoly).lower()\n",
    "second_verb = random.choice(action_verbs).lower()\n",
    "pronoun = random.choice([\"you\",\"your\"]).lower()\n",
    "\n",
    "## once locations available, randomly select location\n",
    "LOCATION = \"Dominic Toretto Avenue\"\n",
    "number = 2000\n",
    "\n",
    "select_second_verb = random.choice([0,1])\n",
    "select_pronoun = random.choice([0,1])\n",
    "select_location = random.choice([0,1])\n",
    "\n",
    "print(\"\\nfirst_verb:\", first_verb, \"\\nsecond_verb:\", second_verb, \"\\npronoun:\",  pronoun, \"\\nLOCATION:\",LOCATION, \"\\nnumber:\",number )\n",
    "\n",
    "print(\"\\nselect_second_verb:\", select_second_verb, \"\\nselect_pronoun:\", select_pronoun, \"\\nselect_location:\",  select_location,)\n",
    "\n",
    "if select_second_verb and select_pronoun and select_location:\n",
    "    keyword_list = [first_verb, second_verb, pronoun, LOCATION]\n",
    "elif select_second_verb == 0 and select_pronoun and select_location:\n",
    "    keyword_list = [first_verb, pronoun, LOCATION]\n",
    "elif select_second_verb == 0 and select_pronoun == 0 and select_location:\n",
    "    keyword_list = [first_verb, LOCATION]\n",
    "elif select_second_verb == 0 and select_pronoun == 1 and select_location == 0:\n",
    "    keyword_list = [first_verb, pronoun, number]\n",
    "elif select_second_verb == 0 and select_pronoun == 0 and select_location == 1:\n",
    "    keyword_list = [first_verb, LOCATION]\n",
    "elif select_second_verb == 1 and select_pronoun == 0 and select_location == 0:\n",
    "    keyword_list = [first_verb, second_verb]\n",
    "elif select_second_verb == 1 and select_pronoun == 0 and select_location == 1:\n",
    "    keyword_list = [first_verb, second_verb, LOCATION]\n",
    "elif select_second_verb == 1 and select_pronoun == 1 and select_location == 0:\n",
    "    keyword_list = [first_verb, second_verb, pronoun]\n",
    "elif select_second_verb == 0 and select_pronoun == 0 and select_location == 0:\n",
    "    keyword_list = [first_verb, number]\n",
    "\n",
    "prison = \"Los Angeles\"\n",
    "if LOCATION == prison:\n",
    "    keyword_list = [LOCATION, \"not pass\", \"not collect\"]\n",
    "print(keyword_list)\n",
    "\n",
    "keyword_string = \"\"\n",
    "\n",
    "for item in keyword_list:\n",
    "    if keyword_string == \"\":\n",
    "        keyword_string += str(item)\n",
    "    else:\n",
    "        keyword_string += \", \" + str(item)\n",
    "        \n",
    "print(keyword_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "6ec845eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "being released, see, Dominic Toretto Avenue\n",
      "See Dominic Toretto. Go there if you have the opportunity.\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens':25,  # number of generated tokens\n",
    "    'temperature': 1,   # controlling the randomness of generations\n",
    "    'end_sequence': \"###\" # stopping sequence for generation\n",
    "}\n",
    "\n",
    "prompt = prompt_text + \"\\nkey: \" + keyword_string + \"\\ntweet:\"\n",
    "#prompt = \"key: move, Schlossallee\\ntweet: Move forward to Schlossallee.\\n###key: draw, Harry Potter, 10, fields, back\\ntweet: Draw a picture of Harry Potter or go 10 fields back. \\n###\\nkey: renovate, pay, houses, bank\\ntweet: Have all your houses renovated! Pay to the bank for each house 1000 Euro.\\n###\\nkey: go, Parkstraße, community ticket\\ntweet: Go back to Parkstraße or take a community ticket. \\n###\\nkey: go, Parkstraße\\ntweet:\"\n",
    "\n",
    "#print(prompt)\n",
    "\n",
    "\n",
    "data = query(prompt,parameters)\n",
    "\n",
    "action_card = re.findall(r\"(?<=tweet:\\s).*\", data)[-1] \n",
    "#print(data)\n",
    "print(keyword_string)\n",
    "print(action_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "fe37ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#action_card = re.match(\"(?<=:$).*\", data)\n",
    "#print(data)\n",
    "#print(action_card)\n",
    "action_card = \"Harry Potter Street is now you; you can do anything in here! Eat.\"\n",
    "action_card = \"You have 2000 dollars.\"\n",
    "action_card = \"Come play Harry with the other wizards. Collect DM 8000,-\"\n",
    "action_card = \"The town of Hogsmeade, the most famous town in Harry Potter, will be closed during the time of the party\"\n",
    "action_card = \"You will be replaced by Harry Potter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "d7985478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advance to the opera square. If you get over Go, collect DM 4000,-\n",
      "You inherit. If you give a house DM 500,- to a player or pay DM 4000,- for a player, then they\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8776719906943025"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if LOCATION == prison:\n",
    "    reference = \"Go to the prison! Go directly there. Do not pass Go. Do not collect DM 4000,-.\"\n",
    "    \n",
    "#action_card = \"Go to the jail in Los Angeles! No Going. Make it through the red line and collect DM 10000,-\"\n",
    "reference = \"Advance to the opera square. If you get over Go, collect DM 4000,-\"\n",
    "print(reference)\n",
    "print(action_card)\n",
    "\n",
    "evaluate_generated_sentence(reference,action_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d1962",
   "metadata": {},
   "source": [
    "## Locations - Use New Data\n",
    "### Preprocess new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "ad9fbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_characters = pd.read_csv(\"tmdb_5000_credits.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "5028ab76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>[{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...</td>\n",
       "      <td>[{\"credit_id\": \"52fe48009251416c750aca23\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>[{\"cast_id\": 4, \"character\": \"Captain Jack Spa...</td>\n",
       "      <td>[{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206647</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>[{\"cast_id\": 1, \"character\": \"James Bond\", \"cr...</td>\n",
       "      <td>[{\"credit_id\": \"54805967c3a36829b5002c41\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49026</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>[{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba...</td>\n",
       "      <td>[{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49529</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>[{\"cast_id\": 5, \"character\": \"John Carter\", \"c...</td>\n",
       "      <td>[{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                     title  \\\n",
       "0     19995                                    Avatar   \n",
       "1       285  Pirates of the Caribbean: At World's End   \n",
       "2    206647                                   Spectre   \n",
       "3     49026                     The Dark Knight Rises   \n",
       "4     49529                               John Carter   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...   \n",
       "1  [{\"cast_id\": 4, \"character\": \"Captain Jack Spa...   \n",
       "2  [{\"cast_id\": 1, \"character\": \"James Bond\", \"cr...   \n",
       "3  [{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba...   \n",
       "4  [{\"cast_id\": 5, \"character\": \"John Carter\", \"c...   \n",
       "\n",
       "                                                crew  \n",
       "0  [{\"credit_id\": \"52fe48009251416c750aca23\", \"de...  \n",
       "1  [{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...  \n",
       "2  [{\"credit_id\": \"54805967c3a36829b5002c41\", \"de...  \n",
       "3  [{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de...  \n",
       "4  [{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de...  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "301cb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_rows = []\n",
    "\n",
    "for malformed_string in movie_characters.cast:\n",
    "    imd_string = list(malformed_string[1:(len(malformed_string)-1)].split(\"}\"))\n",
    "    \n",
    "    new_list = []\n",
    "\n",
    "    for item in imd_string:\n",
    "        try: \n",
    "            if item[0] != \"{\":\n",
    "                item = item[2:(len(item))]\n",
    "            item += \"}\"\n",
    "            new_item =json.loads(item)\n",
    "            person = new_item[\"character\"]\n",
    "            #gender = new_item[\"gender\"]\n",
    "            new_list.append(person)\n",
    "        except IndexError:\n",
    "            break\n",
    "    cast_rows.append(new_list)\n",
    "\n",
    "    \n",
    "cast_dict = {}\n",
    "for movie, cast in zip(movie_characters.title,cast_rows):\n",
    "    cast_dict[movie] = cast\n",
    "\n",
    "    \n",
    "#cast_df = pd.DataFrame(prep_df, columns=[\"movie\",\"cast\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "c59cec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dominic Toretto', \"Brian O'Conner\", 'Hobbs', 'Letty', 'Roman', \"Tej (as Chris 'Ludacris' Bridges)\", 'Mia', 'Jakande', 'Kiet', 'Kara', 'Ramsey', 'Mr. Nobody', 'Deckard Shaw', 'Han', 'Gisele', 'Sean Boswell', 'Elena', 'Hector', 'Sheppard', 'Owen Shaw', 'Safar', 'Jack', 'Jack', 'Samantha Hobbs', 'Letty Fan', 'Female Racer', 'Male Racer', 'Race Starter', 'Hot Teacher', 'Doctor', 'Priest', 'Merc Tech', 'Weapons Tech', 'Billionaire', 'Dominican Priest', 'Hana', 'Merc Driver (as Ben Blankenship)', 'DJ', 'DJ', 'Drone Tech', 'Jasmine', 'Mando', 'Advisor', 'Field Reporter', 'Cop', 'Leo (uncredited / archive)', 'Neela (uncredited / archive)', 'Twinkie (uncredited)', 'Santos (uncredited / archive)', 'Race Wars Racer (uncredited)', \"Brian O'Conner (uncredited)\", \"Brian O'Connor (uncredited)\"]\n"
     ]
    }
   ],
   "source": [
    "cast = cast_dict[\"Furious 7\"]\n",
    "print(cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "d59b6be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'streets': {'1-3': ['Mr. Nobody Street', 'Deckard Shaw Street', 'Han Street'],\n",
       "  '4-6': [],\n",
       "  '7-9': [],\n",
       "  '10-12': [],\n",
       "  '13-15': [],\n",
       "  '16-18': [],\n",
       "  'expensive': ['Dominic Toretto Avenue', \"Brian O'Conner Avenue\"],\n",
       "  'cheap': ['Kiet Drive', 'Kara Drive']},\n",
       " 'stations': ['Letty Station',\n",
       "  'Roman Station',\n",
       "  \"Tej (as Chris 'Ludacris' Bridges) Station\",\n",
       "  'Mia Station'],\n",
       " 'prison': [],\n",
       " 'free_parking': [],\n",
       " 'special': {'1': [], '2': []}}"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_field = {\n",
    "    \"streets\": {\"1-3\": [],\"4-6\": [],\"7-9\":[] , \"10-12\": [], \"13-15\": [], \"16-18\": [], \"expensive\": [], \"cheap\": []},\n",
    "    \"stations\": [],\n",
    "    \"prison\": [],\n",
    "    \"free_parking\": [],\n",
    "    \"special\": {\"1\": [], \"2\": []}\n",
    "}\n",
    "#6*3 streets\n",
    "#2*2 streets\n",
    "#4 * station\n",
    "#1 * prison\n",
    "#1 * free parking\n",
    "#2* elektrizität + wasserwerk\n",
    "\n",
    "new_field[\"streets\"][\"expensive\"] = [x + \" Avenue\" for x in cast[0:2]]\n",
    "new_field[\"streets\"][\"cheap\"] = [x + \" Drive\" for x in cast[8:10]]\n",
    "new_field[\"streets\"][\"1-3\"] = [x + \" Street\" for x in cast[11:14]]\n",
    "new_field[\"stations\"] = [x + \" Station\" for x in cast[3:7]]\n",
    "\n",
    "new_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "6c059c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_names = ['Avenue', 'Park', 'Street', 'Boulevard', 'Road', 'Main Street', 'Drive', 'Lane', 'Alley']\n",
    "station_name = 'Station'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "b48dc701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'streets': {'1-3': ['Mr. Nobody Street', 'Deckard Shaw Street', 'Han Street'],\n",
       "  '4-6': [],\n",
       "  '7-9': [],\n",
       "  '10-12': [],\n",
       "  '13-15': [],\n",
       "  '16-18': [],\n",
       "  'expensive': ['Dominic Toretto Avenue', \"Brian O'Conner Avenue\"],\n",
       "  'cheap': ['Kiet Drive', 'Kara Drive']},\n",
       " 'stations': ['Letty Station',\n",
       "  'Roman Station',\n",
       "  \"Tej (as Chris 'Ludacris' Bridges) Station\",\n",
       "  'Mia Station'],\n",
       " 'prison': [],\n",
       " 'free_parking': [],\n",
       " 'special': {'1': [], '2': []}}"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f993b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "ad4ad995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a special event in the movie Furious 7?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'WikipediaPageSection' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jc/wfw7hm3d4_999_6bt6flqglw0000gn/T/ipykernel_46702/2978768592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQA_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# Convert inputs to features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"`{k}` cannot be empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mQuestionAnsweringPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{item} argument needs to be of type (SquadExample, dict)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36mcreate_sample\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSquadExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSquadExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     def _sanitize_parameters(\n",
      "\u001b[0;32m/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/data/processors/squad.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, qas_id, question_text, context_text, answer_text, start_position_character, title, answers, is_impossible)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# Split on whitespace so that different tokens may be attributed to their original position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0mprev_is_whitespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'WikipediaPageSection' object is not iterable"
     ]
    }
   ],
   "source": [
    "question = \"What is a special event in the movie \" + topic + \"?\"\n",
    "print(question)\n",
    "# a) Get predictions\n",
    "#nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': question,\n",
    "    'context': topic_text\n",
    "}\n",
    "\n",
    "res = nlp(QA_input)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "17d063e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atlanta'"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "c0be7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_field[\"prison\"] = \"Los Angeles\"\n",
    "new_field[\"free_parking\"] = \"Atlanta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d2698",
   "metadata": {},
   "source": [
    "special_1 = \"What is an import monument in the movie \"\n",
    "special_2 = \"What is an expensive location in the movie \"\n",
    "prison = \"Which one is the worst area in the movie \"\n",
    "free_parking = \"What is the loveliest place in the movie \"\n",
    "special_= \"What is a special event in the movie \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "3793b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_only = wiki_page.sections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77c52f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
