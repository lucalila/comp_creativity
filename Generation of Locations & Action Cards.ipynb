{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed467f57",
   "metadata": {},
   "source": [
    "# 1. Generation of Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fabf7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lauraluckert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lauraluckert/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import wikipediaapi\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86809931",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"~/Desktop/\"\n",
    "FILENAME = \"tmdb_5000_credits.csv\"\n",
    "\n",
    "full_path = os.path.expanduser(PATH)\n",
    "os.chdir(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed119dc3",
   "metadata": {},
   "source": [
    "### 0. Example Structure for New Game Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9d1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_field = {\n",
    "    \"streets\": {\"1-3\": [],\"4-6\": [],\"7-9\":[] , \"10-12\": [], \\\n",
    "                \"13-15\": [], \"16-18\": [], \"expensive\": [], \"cheap\": []},\n",
    "    \"stations\": [],\n",
    "    \"prison\": [],\n",
    "    \"free_parking\": [],\n",
    "    \"special\": {\"1\": [], \"2\": []}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c136e82",
   "metadata": {},
   "source": [
    "### 1. Clean and read-in Movie Data\n",
    "Data Source:\n",
    "https://www.kaggle.com/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b229311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_movie_dataset(movie_data):\n",
    "    \"\"\"\n",
    "    :movie_data: Pandas DataFrame holding movie titles, character cast\n",
    "    Preprocess the data, we only need the characters from the movie in a dict\n",
    "    \n",
    "    :returns: dictionary with key = movie title, value = \n",
    "    \"\"\"\n",
    "    \n",
    "    cast_rows = []\n",
    "\n",
    "    for malformed_string in movie_data.cast:\n",
    "        imd_string = list(malformed_string[1:(len(malformed_string)-1)].split(\"}\"))\n",
    "    \n",
    "        new_list = []\n",
    "\n",
    "        for item in imd_string:\n",
    "            try: \n",
    "                if item[0] != \"{\":\n",
    "                    item = item[2:(len(item))]\n",
    "                item += \"}\"\n",
    "                new_item =json.loads(item)\n",
    "                person = new_item[\"character\"]\n",
    "                #gender = new_item[\"gender\"]\n",
    "                new_list.append(person)\n",
    "            except IndexError:\n",
    "                break\n",
    "        cast_rows.append(new_list)\n",
    "\n",
    "    \n",
    "    cast_dict = {}\n",
    "    for movie, cast in zip(movie_data.title,cast_rows):\n",
    "        cast_dict[movie] = cast\n",
    "    \n",
    "    return cast_dict\n",
    "\n",
    "## preprocess dataset\n",
    "movie_characters = pd.read_csv(FILENAME, sep=\",\")\n",
    "cast_dict = clean_movie_dataset(movie_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba3e0c",
   "metadata": {},
   "source": [
    "### 3. Random Selection of Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cbdb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Furious 7 ['Dominic Toretto', \"Brian O'Conner\", 'Hobbs', 'Letty', 'Roman', \"Tej (as Chris 'Ludacris' Bridges)\", 'Mia', 'Jakande', 'Kiet', 'Kara', 'Ramsey', 'Mr. Nobody', 'Deckard Shaw', 'Han', 'Gisele', 'Sean Boswell', 'Elena', 'Hector', 'Sheppard', 'Owen Shaw', 'Safar', 'Jack', 'Jack', 'Samantha Hobbs', 'Letty Fan', 'Female Racer', 'Male Racer', 'Race Starter', 'Hot Teacher', 'Doctor', 'Priest', 'Merc Tech', 'Weapons Tech', 'Billionaire', 'Dominican Priest', 'Hana', 'Merc Driver (as Ben Blankenship)', 'DJ', 'DJ', 'Drone Tech', 'Jasmine', 'Mando', 'Advisor', 'Field Reporter', 'Cop', 'Leo (uncredited / archive)', 'Neela (uncredited / archive)', 'Twinkie (uncredited)', 'Santos (uncredited / archive)', 'Race Wars Racer (uncredited)', \"Brian O'Conner (uncredited)\", \"Brian O'Connor (uncredited)\"]\n"
     ]
    }
   ],
   "source": [
    "## get random movie from our dictionary\n",
    "random_key = random.choice(list(cast_dict))\n",
    "\n",
    "## topic and cast is selected\n",
    "topic = random_key\n",
    "cast = cast_dict[topic]\n",
    "\n",
    "## for testing purposes, we set the topic manually\n",
    "topic = \"Furious 7\"\n",
    "cast = cast_dict[topic]\n",
    "print(topic, cast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e0dde",
   "metadata": {},
   "source": [
    "### 4. Select Characters as New Locations\n",
    "#### 4.1 *To Do*:  Cleaning of character names (no brakets in names etc.)\n",
    "#### 4.2 *To Do*: Useful combination of streetnames with selected characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ec73853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'streets': {'1-3': ['Mr. Nobody Boulevard', 'Deckard Shaw Lane', 'Han Road'], '4-6': ['Sean Boswell Alley', 'Elena Alley', 'Hector Main Street'], '7-9': ['Owen Shaw Boulevard', 'Safar Road', 'Jack Park'], '10-12': ['Samantha Hobbs Alley', 'Letty Fan Alley', 'Female Racer Park'], '13-15': ['Race Starter Main Street', 'Hot Teacher Lane', 'Doctor Street'], '16-18': ['Merc Tech Road', 'Weapons Tech Road'], 'expensive': ['Dominic Toretto Avenue', \"Brian O'Conner Avenue\"], 'cheap': ['Kiet Drive', 'Kara Drive']}, 'stations': ['Letty Station', 'Roman Station', \"Tej (as Chris 'Ludacris' Bridges) Station\", 'Mia Station'], 'prison': 'Walker', 'free_parking': 'Tokyo', 'special': {'1': \"God's Eye\", '2': 'Los Angeles'}}\n"
     ]
    }
   ],
   "source": [
    "## possible street names for combination with characters\n",
    "street_names = [#'Avenue', \n",
    "                'Park', 'Street', 'Boulevard', 'Road', 'Main Street', 'Drive', 'Lane', 'Alley']\n",
    "\n",
    "## fill location entries with characters from film cast (cast_dict)\n",
    "## example default combination\n",
    "new_field[\"streets\"][\"expensive\"] = [x + \" Avenue\" for x in cast[0:2]]\n",
    "new_field[\"streets\"][\"cheap\"] = [x + \" Drive\" for x in cast[8:10]]\n",
    "new_field[\"streets\"][\"1-3\"] = [x + \" \" + random.choice(street_names) for x in cast[11:14]]\n",
    "new_field[\"streets\"][\"4-6\"] = [x + \" \" + random.choice(street_names) for x in cast[15:18]]\n",
    "new_field[\"streets\"][\"7-9\"] = [x + \" \" + random.choice(street_names) for x in cast[19:22]]\n",
    "new_field[\"streets\"][\"10-12\"] = [x + \" \" + random.choice(street_names) for x in cast[23:26]]\n",
    "new_field[\"streets\"][\"13-15\"] = [x + \" \" + random.choice(street_names) for x in cast[27:30]]\n",
    "new_field[\"streets\"][\"16-18\"] = [x + \" \" + random.choice(street_names) for x in cast[31:33]]\n",
    "new_field[\"stations\"] = [x + \" Station\" for x in cast[3:7]]\n",
    "\n",
    "print(new_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037404a7",
   "metadata": {},
   "source": [
    "### 5. Question Answering to Select Characters/Locations for Special Places\n",
    "#### 5.1 Get Wikipedia Data as Q&A Basis Data\n",
    "https://pypi.org/project/Wikipedia-API/0.3.5/\n",
    "- *To Do* : Select only \"Plot\" Section from Wikipedia Data/Find a way to get relevant data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02cc5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic is ok.\n"
     ]
    }
   ],
   "source": [
    "## for regular text output\n",
    "wiki_en_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "\n",
    "## check if page for topic exists\n",
    "if wiki_en_wiki.page(topic).exists():\n",
    "    print(\"Topic is ok.\")\n",
    "    wiki_page = wiki_en_wiki.page(topic)\n",
    "    topic_text = wiki_page.text\n",
    "else:\n",
    "    print(\"Find a new topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e067a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "q_a = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220e6ab",
   "metadata": {},
   "source": [
    "#### 5.2 Select Questions for Q&A model to get wikipedia responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e90f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare questions (examples, to discuss)\n",
    "question_dict = {\"special_1\": \"What is an important monument in the movie \",\n",
    "                \"special_2\": \"What is an expensive location in the movie \",\n",
    "                \"prison\": \"Which one is a tragic area in the movie \",\n",
    "                \"free_parking\": \"What is the loveliest place in the movie \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d54bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is an import monument in the movie Furious 7?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:705: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      "/opt/anaconda3/envs/comp_creativity/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:295: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  p_mask = np.asarray(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.7775615453720093, 'start': 4598, 'end': 4607, 'answer': \"God's Eye\"}\n",
      "What is an expensive location in the movie Furious 7?\n",
      "{'score': 0.8790752291679382, 'start': 4684, 'end': 4695, 'answer': 'Los Angeles'}\n",
      "Which one is a tragic area in the movie Furious 7?\n",
      "{'score': 0.7376387715339661, 'start': 29316, 'end': 29322, 'answer': 'Walker'}\n",
      "What is the loveliest place in the movie Furious 7?\n",
      "{'score': 0.568051815032959, 'start': 3437, 'end': 3442, 'answer': 'Tokyo'}\n"
     ]
    }
   ],
   "source": [
    "for category, question_body in question_dict.items():\n",
    "    question = question_body + topic + \"?\"\n",
    "    print(question)\n",
    "    \n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': topic_text\n",
    "    }\n",
    "    response = q_a(QA_input)\n",
    "    print(response)\n",
    "    \n",
    "    if category == \"special_1\":\n",
    "        new_field[\"special\"][\"1\"] = response[\"answer\"]\n",
    "    elif category == \"special_2\":\n",
    "        new_field[\"special\"][\"2\"] = response[\"answer\"]\n",
    "    else:\n",
    "        new_field[category] = response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3378ac",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluate Responses:\n",
    "- To Do: Check if location/character already exists in new_field\n",
    "- To Do: filter for bad scores, retrigger question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b20dedbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'streets': {'1-3': ['Mr. Nobody Boulevard', 'Deckard Shaw Lane', 'Han Road'],\n",
       "  '4-6': ['Sean Boswell Alley', 'Elena Alley', 'Hector Main Street'],\n",
       "  '7-9': ['Owen Shaw Boulevard', 'Safar Road', 'Jack Park'],\n",
       "  '10-12': ['Samantha Hobbs Alley', 'Letty Fan Alley', 'Female Racer Park'],\n",
       "  '13-15': ['Race Starter Main Street', 'Hot Teacher Lane', 'Doctor Street'],\n",
       "  '16-18': ['Merc Tech Road', 'Weapons Tech Road'],\n",
       "  'expensive': ['Dominic Toretto Avenue', \"Brian O'Conner Avenue\"],\n",
       "  'cheap': ['Kiet Drive', 'Kara Drive']},\n",
       " 'stations': ['Letty Station',\n",
       "  'Roman Station',\n",
       "  \"Tej (as Chris 'Ludacris' Bridges) Station\",\n",
       "  'Mia Station'],\n",
       " 'prison': 'Walker',\n",
       " 'free_parking': 'Tokyo',\n",
       " 'special': {'1': \"God's Eye\", '2': 'Los Angeles'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e457f",
   "metadata": {},
   "source": [
    "# 2. Generation of Action Cards "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c3b59",
   "metadata": {},
   "source": [
    "### 1. Plagiarism: Read in Monopoly Data\n",
    "- Get action verbs from monopoly data\n",
    "- use real action cards for few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce1b0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_MONOPOLY = \"monopoly_action_cards_keywords.csv\"\n",
    "monopoly_data = pd.read_csv(FILENAME_MONOPOLY, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b0ef7",
   "metadata": {},
   "source": [
    "#### 1.1 Keyword Preparation from Monopoly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5966595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic's keyword_list is:  ['Pay', 'take', 'come', '..', 'Go', 'get', '..', 'pays', 'are', 'pays', 'Go', 'receive', 'inherit', 'receive', '..', 'is', 'win', 'won', 'is', '..', 'receives', 'has', 'buy', 'get', 'have', 'been', 'elected', 'Have', 'renovated', 'Euro', 'be', 'called', 'do', 'Pay', 'Do', 'pass', 'collect', 'be', 'released', 'keep', 'need', 'sell', 'Do', 'pass', 'collect', 'be', 'released', 'keep', 'need', 'sell']\n"
     ]
    }
   ],
   "source": [
    "## get pos tags\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "## Get action verbs from real monopoly action cards\n",
    "text_data = \"\"\n",
    "for text_item in monopoly_data[\"content\"]:\n",
    "    text_data += \". \" + text_item\n",
    "    \n",
    "inspect_actions = preprocess(text_data)\n",
    "\n",
    "## use only verbs\n",
    "keyword_list_verbs = []\n",
    "\n",
    "for pos_tag in inspect_actions:\n",
    "    if re.match(\"VB.*\", pos_tag[1]):\n",
    "        if pos_tag[0] == \"DM\":\n",
    "            keyword_list_verbs.append(\"Euro\")\n",
    "        else:\n",
    "            keyword_list_verbs.append(pos_tag[0])  \n",
    "        \n",
    "print(\"The topic's keyword_list is: \", keyword_list_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526302ca",
   "metadata": {},
   "source": [
    "Data Source Further Action Words:\n",
    "https://www.citationmachine.net/resources/grammar-guides/verb/list-verbs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22c90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tag == \"VB.*\" from real monopoly action cards\n",
    "action_verbs_monopoly = [\"Pay\",\"Take\",\"Come\",\"Go\",\"Get\",\"Receive\",\"Inherit\",\"Win\",\"Pass\",\n",
    "                         \"Collect\",\"being released\",\"Keep\",\"Sell\"]\n",
    "action_verbs = [\"Act\",\"Answer\",\"Approve\",\"Arrange\",\"Break\",\"Build\",\"Buy\",\"Coach\",\"Color\",\"Cough\",\"Create\", \n",
    "                \"Complete\",\"Cry\",\"Dance\",\"Describe\",\"Draw\",\"Drink\",\"Eat\",\"Edit\",\"Enter\",\"Exit\",\n",
    "                \"Imitate\",\"Invent\",\"Jump\",\"Laugh\",\"Lie\",\"Listen\",\"Paint\",\"Plan\",\"Play\",\"Read\",\"Replace\",\n",
    "                \"Run\",\"Scream\",\"See\",\"Shop\",\"Shout\",\"Sing\",\"Skip\",\"Sleep\",\"Sneeze\",\"Solve\",\"Study\",\"Teach\",\n",
    "                \"Touch\",\"Turn\",\"Walk\",\"Win\",\"Write\",\"Whistle\",\"Yank\",\"Zip\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7ca3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## locations into flat list\n",
    "locations = []\n",
    "for _, value in new_field[\"streets\"].items():\n",
    "    for item in value:\n",
    "        locations.append(item)\n",
    "for _, value in new_field[\"special\"].items():\n",
    "    for item in value:\n",
    "        locations.append(item)\n",
    "for item in new_field[\"stations\"]:\n",
    "    locations.append(item)\n",
    "locations.append(new_field[\"prison\"])\n",
    "locations.append(new_field[\"free_parking\"])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebab3e4",
   "metadata": {},
   "source": [
    "#### 1.2 Few-Shot Learning Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4828d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate few shot training data for text generation\n",
    "prompt_text = \"\"\n",
    "\n",
    "for text, keywords in zip(monopoly_data[\"content\"], monopoly_data[\"keywords\"]):\n",
    "    imd = \"key: \" + keywords + \"\\ntweet: \" + text + \"\\n###\"\n",
    "    prompt_text += imd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c93889",
   "metadata": {},
   "source": [
    "### 2. Random Keyword Generation for New Action Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9b86cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first_verb: take \n",
      "second_verb: dance \n",
      "pronoun: you \n",
      "LOCATION: Female Racer Park \n",
      "number: 2000\n",
      "\n",
      "select_second_verb: 0 \n",
      "select_pronoun: 0 \n",
      "select_location: 0\n",
      "take, 2000\n"
     ]
    }
   ],
   "source": [
    "## once locations available, randomly select location\n",
    "#LOCATION = \"Dominic Toretto Avenue\"\n",
    "\n",
    "## randomly select verbs, pronouns, locations, fixed number\n",
    "first_verb = random.choice(action_verbs_monopoly).lower()\n",
    "second_verb = random.choice(action_verbs).lower()\n",
    "pronoun = random.choice([\"you\",\"your\",\"yours\"]).lower()\n",
    "LOCATION = random.choice(locations)\n",
    "number = 2000\n",
    "\n",
    "## special case for prison\n",
    "if LOCATION == new_field[\"prison\"]:\n",
    "    keyword_list = [LOCATION, \"not pass\", \"not collect\"]\n",
    "\n",
    "else:\n",
    "    ## randomly select if second verb, pronoun and location should be considered\n",
    "    select_second_verb = random.choice([0,1])\n",
    "    select_pronoun = random.choice([0,1])\n",
    "    select_location = random.choice([0,1])\n",
    "\n",
    "    print(\"\\nfirst_verb:\", first_verb, \"\\nsecond_verb:\", second_verb, \"\\npronoun:\",  \\\n",
    "          pronoun, \"\\nLOCATION:\",LOCATION, \"\\nnumber:\",number )\n",
    "\n",
    "    print(\"\\nselect_second_verb:\", select_second_verb, \"\\nselect_pronoun:\", select_pronoun, \\\n",
    "          \"\\nselect_location:\",  select_location,)\n",
    "\n",
    "    if select_second_verb and select_pronoun and select_location:\n",
    "        keyword_list = [first_verb, second_verb, pronoun, LOCATION]\n",
    "    elif select_second_verb == 0 and select_pronoun and select_location:\n",
    "        keyword_list = [first_verb, pronoun, LOCATION]\n",
    "    elif select_second_verb == 0 and select_pronoun == 0 and select_location:\n",
    "        keyword_list = [first_verb, LOCATION]\n",
    "    elif select_second_verb == 0 and select_pronoun == 1 and select_location == 0:\n",
    "        keyword_list = [first_verb, pronoun, number]\n",
    "    elif select_second_verb == 0 and select_pronoun == 0 and select_location == 1:\n",
    "        keyword_list = [first_verb, LOCATION]\n",
    "    elif select_second_verb == 1 and select_pronoun == 0 and select_location == 0:\n",
    "        keyword_list = [first_verb, second_verb]\n",
    "    elif select_second_verb == 1 and select_pronoun == 0 and select_location == 1:\n",
    "        keyword_list = [first_verb, second_verb, LOCATION]\n",
    "    elif select_second_verb == 1 and select_pronoun == 1 and select_location == 0:\n",
    "        keyword_list = [first_verb, second_verb, pronoun]\n",
    "    elif select_second_verb == 0 and select_pronoun == 0 and select_location == 0:\n",
    "        keyword_list = [first_verb, number]\n",
    "\n",
    "\n",
    "keyword_string = \"\"\n",
    "\n",
    "for item in keyword_list:\n",
    "    if keyword_string == \"\":\n",
    "        keyword_string += str(item)\n",
    "    else:\n",
    "        keyword_string += \", \" + str(item)\n",
    "        \n",
    "print(keyword_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62fb8a",
   "metadata": {},
   "source": [
    "### 2. Few-Shot Learning Key-to-Text Generation for Action Cards\n",
    "Source Inference API: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afbd3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "## api token can be generated via free huggingface account\n",
    "API_TOKEN = \"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\"\n",
    "\n",
    "def query(payload='',parameters=None,options={'use_cache': False}):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    body = {\"inputs\":payload,'parameters':parameters,'options':options}\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data= json.dumps(body))\n",
    "    try:\n",
    "      response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        return \"Error:\"+\" \".join(response.json()['error'])\n",
    "    else:\n",
    "      return response.json()[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "931b7737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take, 2000\n",
      "Take 2000,000 from the bank. \n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens':25,  # number of generated tokens\n",
    "    'temperature': 1,   # controlling the randomness of generations\n",
    "    'end_sequence': \"###\" # stopping sequence for generation\n",
    "}\n",
    "\n",
    "prompt = prompt_text + \"\\nkey: \" + keyword_string + \"\\ntweet:\"\n",
    "\n",
    "\n",
    "data = query(prompt,parameters)\n",
    "\n",
    "action_card = re.findall(r\"(?<=tweet:\\s).*\", data)[-1] \n",
    "#print(data)\n",
    "print(keyword_string)\n",
    "print(action_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7aa2ca",
   "metadata": {},
   "source": [
    "### 3. Evaluation of Generated Action Card\n",
    "\n",
    "- To Do: Which action cards should be used as reference for which input tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff14f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_distribution(pos_tuples_of_sentence):\n",
    "    \"\"\"\n",
    "    :pos_tuple_of_sentences: tuple (token, pos_tag) as returned from preprocess function\n",
    "    \n",
    "    crop pos tags into relevant groups (first two letters)\n",
    "    count occurences of pos tags in input sentence\n",
    "    \n",
    "    :returns: pandas DataFrame with pos_tag and its frequency\n",
    "    \n",
    "    \"\"\"\n",
    "    pos_df = pd.DataFrame(pos_tuples_of_sentence,columns=[\"token\",\"long_pos_tag\"])\n",
    "    pos_df[\"pos_tag\"] = [x[0:2] for x in pos_df[\"long_pos_tag\"]]\n",
    "    freq_df = pos_df[\"pos_tag\"].value_counts()\n",
    "    \n",
    "    return freq_df\n",
    "\n",
    "def evaluate_generated_sentence(reference, new_sentence):\n",
    "\n",
    "    ## preprocess both\n",
    "    reference = preprocess(reference)\n",
    "    new_sentence = preprocess(new_sentence)\n",
    "    \n",
    "    ## pos distribution\n",
    "    reference = pos_distribution(reference)\n",
    "    new_sentence = pos_distribution(new_sentence)\n",
    "    \n",
    "    ## merge vectors\n",
    "    merged_df = pd.merge(reference,new_sentence,how=\"outer\", left_index=True,right_index=True).fillna(0)\n",
    "    merged_df.columns=[\"reference\",\"target\"]\n",
    "    \n",
    "    ## calc cosine similarity \n",
    "    cos_similarity = 1 - cosine(merged_df[\"reference\"], merged_df[\"target\"])\n",
    "    \n",
    "    ## calc scalar product between vectors\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9dc4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
